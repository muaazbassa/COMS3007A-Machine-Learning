{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Increase the complexity of the CNN and Ensemble methods\n",
    "#### Ensemble methods: Instead of training a single model, you can try using ensemble methods such as bagging or boosting. These techniques involve training multiple models and combining their predictions, which often leads to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model 1 Training:\n",
      "Epoch 1/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.2569 - accuracy: 0.1474\n",
      "Epoch 2/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.1798 - accuracy: 0.1759\n",
      "Epoch 3/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.1251 - accuracy: 0.1886\n",
      "Epoch 4/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.0772 - accuracy: 0.2126\n",
      "Epoch 5/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.0314 - accuracy: 0.2298\n",
      "Epoch 6/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.9763 - accuracy: 0.2547\n",
      "Epoch 7/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.9214 - accuracy: 0.2822\n",
      "Epoch 8/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.8747 - accuracy: 0.2994\n",
      "Epoch 9/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.8383 - accuracy: 0.3136\n",
      "Epoch 10/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.8041 - accuracy: 0.3267\n",
      "Epoch 11/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.7609 - accuracy: 0.3363\n",
      "Epoch 12/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.7376 - accuracy: 0.3432\n",
      "Epoch 13/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.7055 - accuracy: 0.3541\n",
      "Epoch 14/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.6868 - accuracy: 0.3622\n",
      "Epoch 15/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.6719 - accuracy: 0.3664\n",
      "Epoch 16/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.6453 - accuracy: 0.3747\n",
      "Epoch 17/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.6235 - accuracy: 0.3829\n",
      "Epoch 18/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.6075 - accuracy: 0.3880\n",
      "Epoch 19/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.5853 - accuracy: 0.3939\n",
      "Epoch 20/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.5625 - accuracy: 0.4007\n",
      "Epoch 21/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.5450 - accuracy: 0.4130\n",
      "Epoch 22/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.5161 - accuracy: 0.4212\n",
      "Epoch 23/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.5105 - accuracy: 0.4230\n",
      "Epoch 24/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.5039 - accuracy: 0.4212\n",
      "Epoch 25/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.4938 - accuracy: 0.4271\n",
      "Epoch 26/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.4724 - accuracy: 0.4349\n",
      "Epoch 27/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.4534 - accuracy: 0.4418\n",
      "Epoch 28/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.4517 - accuracy: 0.4440\n",
      "Epoch 29/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.4483 - accuracy: 0.4415\n",
      "Epoch 30/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.4315 - accuracy: 0.4515\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 69, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 34, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 32, 64)            6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 16, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                32800     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,466\n",
      "Trainable params: 39,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Accuracy: 0.22733333333333333\n",
      "Model Loss: 1.431456208229065\n",
      "\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "Ensemble Model Accuracy: 0.22733333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the training data and labels\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Assuming X_train is a Pandas Series\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "X_train = X_train.values.reshape(-1, 71, 1)\n",
    "X_test = X_test.values.reshape(-1, 71, 1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71, 1)  # Number of input features and channels\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.001  # Adjust as necessary\n",
    "NUM_MODELS = 1  # Number of models in the ensemble\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the ensemble of models\n",
    "ensemble_models = []\n",
    "for _ in range(NUM_MODELS):\n",
    "    # Define the CNN architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Compile the model with the custom optimizer\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    # Append the model to the ensemble\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "# Train each model in the ensemble\n",
    "for model_index, model in enumerate(ensemble_models):\n",
    "    print(f\"Model {model_index + 1} Training:\")\n",
    "    history = model.fit(X_train, y_train_encoded,\n",
    "                        epochs=30,\n",
    "                        batch_size=32,\n",
    "                        verbose=1)  # Set verbose=1 to see training progress\n",
    "\n",
    "    # Predicting the test set results\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model Accuracy:', accuracy)\n",
    "    print('Model Loss:', history.history['loss'][-1])\n",
    "    print()\n",
    "\n",
    "# Predicting the test set results using the ensemble\n",
    "y_test_preds = []\n",
    "for model in ensemble_models:\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "# Take the majority vote from the ensemble predictions\n",
    "y_test_preds_ensemble = np.round(np.mean(y_test_preds, axis=0)).astype(int)\n",
    "\n",
    "# Calculate accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, y_test_preds_ensemble)\n",
    "print('Ensemble Model Accuracy:', ensemble_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_123 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,754\n",
      "Trainable params: 87,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "219/219 [==============================] - 1s 1ms/step - loss: 0.0899 - accuracy: 0.1102\n",
      "Epoch 2/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.1507\n",
      "Epoch 3/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.1647\n",
      "Epoch 4/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.1961\n",
      "Epoch 5/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.2219\n",
      "Epoch 6/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.2389\n",
      "Epoch 7/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.2664\n",
      "Epoch 8/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.2889\n",
      "Epoch 9/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3059\n",
      "Epoch 10/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.3209\n",
      "Epoch 11/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3377\n",
      "Epoch 12/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.3515\n",
      "Epoch 13/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3556\n",
      "Epoch 14/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.3702\n",
      "Epoch 15/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.3724\n",
      "Epoch 16/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.3807\n",
      "Epoch 17/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3946\n",
      "Epoch 18/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3979\n",
      "Epoch 19/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.4145\n",
      "Epoch 20/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.4174\n",
      "Epoch 21/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.4209\n",
      "Epoch 22/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.4283\n",
      "Epoch 23/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.4410\n",
      "Epoch 24/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.4474\n",
      "Epoch 25/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.4606\n",
      "Epoch 26/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.4649\n",
      "Epoch 27/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.4724\n",
      "Epoch 28/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.4794\n",
      "Epoch 29/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.4894\n",
      "Epoch 30/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.4928\n",
      "94/94 [==============================] - 0s 706us/step\n",
      "Model accuracy: 0.3516666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rotate Data Function\n",
    "def rotate_data(data, angle):\n",
    "    rotated_data = []\n",
    "    for image in data:\n",
    "        rotated_image = np.concatenate((image[angle:], image[:angle]))\n",
    "        rotated_data.append(rotated_image)\n",
    "    return np.array(rotated_data)\n",
    "\n",
    "# Add Noise Function\n",
    "def add_noise(data, mean, std_dev):\n",
    "    noisy_data = data + np.random.normal(mean, std_dev, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Normalize the input data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train[i]\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to NumPy arrays\n",
    "X_train = np.array(augmented_X_train)\n",
    "y_train = np.array(augmented_y_train)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Rotate the data\n",
    "X_train = rotate_data(X_train, angle=10)\n",
    "X_test = rotate_data(X_test, angle=10)\n",
    "\n",
    "# Add noise to the data\n",
    "X_train = add_noise(X_train, 0, 0.1)\n",
    "X_test = add_noise(X_test, 0, 0.1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.001  # Adjust as necessary\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the NN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define an Adam optimizer with the desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs=30,\n",
    "                    batch_size=64)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_test_pred_prob = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Model accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_602 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_502 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_603 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_503 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_402 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_303 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_604 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_504 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_403 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_304 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_605 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_505 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_404 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_305 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_606 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_506 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_405 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_607 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "165/165 [==============================] - 3s 11ms/step - loss: 0.0903 - accuracy: 0.1444\n",
      "Epoch 2/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0835 - accuracy: 0.2572\n",
      "Epoch 3/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0793 - accuracy: 0.3255\n",
      "Epoch 4/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0763 - accuracy: 0.3731\n",
      "Epoch 5/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0730 - accuracy: 0.4127\n",
      "Epoch 6/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0706 - accuracy: 0.4398\n",
      "Epoch 7/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0692 - accuracy: 0.4560\n",
      "Epoch 8/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0670 - accuracy: 0.4821\n",
      "Epoch 9/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0661 - accuracy: 0.4900\n",
      "Epoch 10/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0643 - accuracy: 0.5130\n",
      "Epoch 11/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0638 - accuracy: 0.5152\n",
      "Epoch 12/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0624 - accuracy: 0.5312\n",
      "Epoch 13/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0614 - accuracy: 0.5416\n",
      "Epoch 14/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0610 - accuracy: 0.5449\n",
      "Epoch 15/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0598 - accuracy: 0.5563\n",
      "Epoch 16/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0586 - accuracy: 0.5705\n",
      "Epoch 17/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0581 - accuracy: 0.5728\n",
      "Epoch 18/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0581 - accuracy: 0.5749\n",
      "Epoch 19/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0575 - accuracy: 0.5793\n",
      "Epoch 20/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0574 - accuracy: 0.5807\n",
      "Epoch 21/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0560 - accuracy: 0.5915\n",
      "Epoch 22/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0560 - accuracy: 0.5929\n",
      "Epoch 23/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0552 - accuracy: 0.6031\n",
      "Epoch 24/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0550 - accuracy: 0.6023\n",
      "Epoch 25/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0548 - accuracy: 0.6047\n",
      "Epoch 26/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0540 - accuracy: 0.6111\n",
      "Epoch 27/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0532 - accuracy: 0.6183\n",
      "Epoch 28/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0523 - accuracy: 0.6267\n",
      "Epoch 29/30\n",
      "165/165 [==============================] - 2s 12ms/step - loss: 0.0534 - accuracy: 0.6150\n",
      "Epoch 30/30\n",
      "165/165 [==============================] - 2s 12ms/step - loss: 0.0517 - accuracy: 0.6324\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.538\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Rotate Data Function\n",
    "def rotate_data(data, angle):\n",
    "    rotated_data = []\n",
    "    for _, row in data.iterrows():\n",
    "        image = row.to_numpy().reshape((71,))\n",
    "        # Perform rotation operation specific to your data shape\n",
    "        # Modify the rotation operation based on your specific requirements\n",
    "        rotated_image = np.concatenate((image[angle:], image[:angle]))\n",
    "        rotated_data.append(rotated_image)\n",
    "    return np.array(rotated_data)\n",
    "\n",
    "# Add Noise Function\n",
    "def add_noise(data, mean, std_dev):\n",
    "    noisy_data = data + np.random.normal(mean, std_dev, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Rotate the data\n",
    "X_train = rotate_data(X_train, angle=10)\n",
    "X_test = rotate_data(X_test, angle=10)\n",
    "\n",
    "\n",
    "# Add noise to the data\n",
    "X_train = add_noise(X_train, 0, 0.1)\n",
    "X_test = add_noise(X_test, 0, 0.1)\n",
    "\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.025  # Adjust as necessary\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the NN architecture\n",
    "# Define the NN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define an Adam optimizer with the desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs=30,\n",
    "                    batch_size=128)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_test_pred_prob = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Model accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m X_test \u001b[39m=\u001b[39m add_noise(X_test, \u001b[39m0\u001b[39m, \u001b[39m0.1\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[39m# Convert the data to numpy arrays\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39;49mto_numpy()\n\u001b[0;32m     73\u001b[0m X_test \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m     74\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mravel(y_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create an ensemble of neural networks\n",
    "ensemble_size = 10\n",
    "ensemble = []\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.025  # Adjust as necessary\n",
    "\n",
    "# Train individual neural network models\n",
    "for _ in range(ensemble_size):\n",
    "    model = tf.keras.Sequential([\n",
    "         tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model accuracy:', accuracy)\n",
    "    \n",
    "    ensemble.append(model)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred_prob = np.zeros((len(y_test), np.unique(y_train).shape[0]))\n",
    "for model in ensemble:\n",
    "    y_test_pred_prob += model.predict(X_test)\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Ensemble accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_86 (Dense)            (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_70 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_71 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_53 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_72 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_54 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_55 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_56 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 3s 3ms/step - loss: 2.3448 - accuracy: 0.1035\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3088 - accuracy: 0.1009\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3116 - accuracy: 0.0966\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3127 - accuracy: 0.1018\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3137 - accuracy: 0.1008\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3149 - accuracy: 0.0988\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3145 - accuracy: 0.1031\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3149 - accuracy: 0.1010\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3145 - accuracy: 0.1033\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3149 - accuracy: 0.1021\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3126 - accuracy: 0.1008\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3128 - accuracy: 0.0999\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3134 - accuracy: 0.1020\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3157 - accuracy: 0.1015\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3132 - accuracy: 0.1010\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3135 - accuracy: 0.0980\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3147 - accuracy: 0.0983\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3147 - accuracy: 0.1017\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3118 - accuracy: 0.0997\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3140 - accuracy: 0.1013\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3130 - accuracy: 0.0995\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3118 - accuracy: 0.1000\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3109 - accuracy: 0.0995\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.3126 - accuracy: 0.0971\n",
      "Epoch 25/30\n",
      " 67/329 [=====>........................] - ETA: 0s - loss: 2.3142 - accuracy: 0.0993"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39m# Summary of the model\u001b[39;00m\n\u001b[0;32m    125\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[1;32m--> 127\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m)\n\u001b[0;32m    129\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m    130\u001b[0m y_test_pred_prob \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Rotate Data Function\n",
    "def rotate_data(data, angle):\n",
    "    rotated_data = np.concatenate((data[:, angle:], data[:, :angle]), axis=1)\n",
    "    return rotated_data\n",
    "\n",
    "# Add Noise Function\n",
    "def add_noise(data, mean, std_dev):\n",
    "    noisy_data = data + np.random.normal(mean, std_dev, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "# Data preprocessing - feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train[i]\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train[i])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train[i])\n",
    "\n",
    "# Convert augmented data to numpy arrays\n",
    "augmented_X_train = np.array(augmented_X_train)\n",
    "augmented_y_train = np.array(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = np.concatenate([X_train, augmented_X_train], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Rotate the data\n",
    "X_train = rotate_data(X_train, angle=10)\n",
    "X_test = rotate_data(X_test, angle=10)\n",
    "\n",
    "# Add noise to the data\n",
    "X_train = add_noise(X_train, 0, 0.1)\n",
    "X_test = add_noise(X_test, 0, 0.1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.025  # Adjust as necessary\n",
    "\n",
    "# Create an ensemble of neural networks\n",
    "ensemble_size = 5\n",
    "ensemble = []\n",
    "\n",
    "# Train individual neural network models\n",
    "for _ in range(ensemble_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model accuracy:', accuracy)\n",
    "\n",
    "    ensemble.append(model)\n",
    "\n",
    "# Make predictions on the test set using the ensemble\n",
    "y_test_pred_prob = np.zeros((len(y_test), NUM_CLASSES))\n",
    "for model in ensemble:\n",
    "    y_test_pred_prob += model.predict(X_test)\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy of the ensemble\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Ensemble accuracy:', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
