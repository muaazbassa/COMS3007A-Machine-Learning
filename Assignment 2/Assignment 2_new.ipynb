{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Increase the complexity of the CNN and Ensemble methods\n",
    "#### Ensemble methods: Instead of training a single model, you can try using ensemble methods such as bagging or boosting. These techniques involve training multiple models and combining their predictions, which often leads to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model 1 Training:\n",
      "Epoch 1/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.2535 - accuracy: 0.1460\n",
      "Epoch 2/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.1251 - accuracy: 0.2115\n",
      "Epoch 3/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.0086 - accuracy: 0.2608\n",
      "Epoch 4/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.8951 - accuracy: 0.3075\n",
      "Epoch 5/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.7780 - accuracy: 0.3473\n",
      "Epoch 6/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.6656 - accuracy: 0.3890\n",
      "Epoch 7/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.5670 - accuracy: 0.4295\n",
      "Epoch 8/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.4826 - accuracy: 0.4550\n",
      "Epoch 9/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.4041 - accuracy: 0.4852\n",
      "Epoch 10/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.3221 - accuracy: 0.5144\n",
      "Epoch 11/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.2557 - accuracy: 0.5405\n",
      "Epoch 12/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.2123 - accuracy: 0.5535\n",
      "Epoch 13/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.1565 - accuracy: 0.5718\n",
      "Epoch 14/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.1232 - accuracy: 0.5832\n",
      "Epoch 15/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.0797 - accuracy: 0.6009\n",
      "Epoch 16/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.0544 - accuracy: 0.6100\n",
      "Epoch 17/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.0223 - accuracy: 0.6189\n",
      "Epoch 18/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.0105 - accuracy: 0.6225\n",
      "Epoch 19/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.9734 - accuracy: 0.6376\n",
      "Epoch 20/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.9470 - accuracy: 0.6454\n",
      "Epoch 21/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.9356 - accuracy: 0.6505\n",
      "Epoch 22/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.9267 - accuracy: 0.6517\n",
      "Epoch 23/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.9051 - accuracy: 0.6621\n",
      "Epoch 24/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.8738 - accuracy: 0.6761\n",
      "Epoch 25/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8727 - accuracy: 0.6717\n",
      "Epoch 26/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8565 - accuracy: 0.6799\n",
      "Epoch 27/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8423 - accuracy: 0.6829\n",
      "Epoch 28/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.8322 - accuracy: 0.6878\n",
      "Epoch 29/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8326 - accuracy: 0.6880\n",
      "Epoch 30/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8053 - accuracy: 0.6997\n",
      "94/94 [==============================] - 0s 964us/step\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_77 (Conv1D)          (None, 69, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_77 (MaxPoolin  (None, 34, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_78 (Conv1D)          (None, 32, 64)            6208      \n",
      "                                                                 \n",
      " max_pooling1d_78 (MaxPoolin  (None, 16, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_36 (Flatten)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 32)                32800     \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,466\n",
      "Trainable params: 39,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Accuracy: 0.25666666666666665\n",
      "Model Loss: 0.8053159117698669\n",
      "\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "Ensemble Model Accuracy: 0.25666666666666665\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the training data and labels\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Assuming X_train is a Pandas Series\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "X_train = X_train.values.reshape(-1, 71, 1)\n",
    "X_test = X_test.values.reshape(-1, 71, 1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71, 1)  # Number of input features and channels\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.001  # Adjust as necessary\n",
    "NUM_MODELS = 1  # Number of models in the ensemble\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the ensemble of models\n",
    "ensemble_models = []\n",
    "for _ in range(NUM_MODELS):\n",
    "    # Define the CNN architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Compile the model with the custom optimizer\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    # Append the model to the ensemble\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "# Train each model in the ensemble\n",
    "for model_index, model in enumerate(ensemble_models):\n",
    "    print(f\"Model {model_index + 1} Training:\")\n",
    "    history = model.fit(X_train, y_train_encoded,\n",
    "                        epochs=30,\n",
    "                        batch_size=32,\n",
    "                        verbose=1)  # Set verbose=1 to see training progress\n",
    "\n",
    "    # Predicting the test set results\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model Accuracy:', accuracy)\n",
    "    print('Model Loss:', history.history['loss'][-1])\n",
    "    print()\n",
    "\n",
    "# Predicting the test set results using the ensemble\n",
    "y_test_preds = []\n",
    "for model in ensemble_models:\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "# Take the majority vote from the ensemble predictions\n",
    "y_test_preds_ensemble = np.round(np.mean(y_test_preds, axis=0)).astype(int)\n",
    "\n",
    "# Calculate accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, y_test_preds_ensemble)\n",
    "print('Ensemble Model Accuracy:', ensemble_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_123 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,754\n",
      "Trainable params: 87,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "219/219 [==============================] - 1s 1ms/step - loss: 0.0899 - accuracy: 0.1102\n",
      "Epoch 2/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.1507\n",
      "Epoch 3/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.1647\n",
      "Epoch 4/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.1961\n",
      "Epoch 5/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.2219\n",
      "Epoch 6/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.2389\n",
      "Epoch 7/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.2664\n",
      "Epoch 8/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.2889\n",
      "Epoch 9/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3059\n",
      "Epoch 10/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.3209\n",
      "Epoch 11/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3377\n",
      "Epoch 12/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.3515\n",
      "Epoch 13/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3556\n",
      "Epoch 14/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.3702\n",
      "Epoch 15/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.3724\n",
      "Epoch 16/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.3807\n",
      "Epoch 17/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3946\n",
      "Epoch 18/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3979\n",
      "Epoch 19/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.4145\n",
      "Epoch 20/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.4174\n",
      "Epoch 21/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.4209\n",
      "Epoch 22/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.4283\n",
      "Epoch 23/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.4410\n",
      "Epoch 24/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.4474\n",
      "Epoch 25/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.4606\n",
      "Epoch 26/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.4649\n",
      "Epoch 27/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.4724\n",
      "Epoch 28/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.4794\n",
      "Epoch 29/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.4894\n",
      "Epoch 30/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.4928\n",
      "94/94 [==============================] - 0s 706us/step\n",
      "Model accuracy: 0.3516666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rotate Data Function\n",
    "def rotate_data(data, angle):\n",
    "    rotated_data = []\n",
    "    for image in data:\n",
    "        rotated_image = np.concatenate((image[angle:], image[:angle]))\n",
    "        rotated_data.append(rotated_image)\n",
    "    return np.array(rotated_data)\n",
    "\n",
    "# Add Noise Function\n",
    "def add_noise(data, mean, std_dev):\n",
    "    noisy_data = data + np.random.normal(mean, std_dev, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Normalize the input data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train[i]\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to NumPy arrays\n",
    "X_train = np.array(augmented_X_train)\n",
    "y_train = np.array(augmented_y_train)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Rotate the data\n",
    "X_train = rotate_data(X_train, angle=10)\n",
    "X_test = rotate_data(X_test, angle=10)\n",
    "\n",
    "# Add noise to the data\n",
    "X_train = add_noise(X_train, 0, 0.1)\n",
    "X_test = add_noise(X_test, 0, 0.1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.001  # Adjust as necessary\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the NN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define an Adam optimizer with the desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs=30,\n",
    "                    batch_size=64)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_test_pred_prob = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Model accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_456 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_380 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_457 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_381 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_304 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_228 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_458 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_382 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_305 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_229 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_459 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_383 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_306 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_230 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_460 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_384 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_307 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_461 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 4s 7ms/step - loss: 0.0899 - accuracy: 0.1414\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0844 - accuracy: 0.2470\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0803 - accuracy: 0.3151\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0766 - accuracy: 0.3711\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0736 - accuracy: 0.4070\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0709 - accuracy: 0.4378\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0700 - accuracy: 0.4482\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0699 - accuracy: 0.4495\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0672 - accuracy: 0.4852\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0661 - accuracy: 0.4946\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0646 - accuracy: 0.5108\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0637 - accuracy: 0.5199\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0629 - accuracy: 0.5274\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0623 - accuracy: 0.5343\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0614 - accuracy: 0.5466\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0608 - accuracy: 0.5513\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0602 - accuracy: 0.5538\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0591 - accuracy: 0.5630\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0588 - accuracy: 0.5695\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0582 - accuracy: 0.5779\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0571 - accuracy: 0.5896\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0570 - accuracy: 0.5875\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0563 - accuracy: 0.5971\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0557 - accuracy: 0.5970\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0554 - accuracy: 0.5996\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0551 - accuracy: 0.6027\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0541 - accuracy: 0.6131\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0552 - accuracy: 0.6030\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0539 - accuracy: 0.6160\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 0.0534 - accuracy: 0.6200\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.545\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Rotate Data Function\n",
    "def rotate_data(data, angle):\n",
    "    rotated_data = []\n",
    "    for _, row in data.iterrows():\n",
    "        image = row.to_numpy().reshape((71,))\n",
    "        # Perform rotation operation specific to your data shape\n",
    "        # Modify the rotation operation based on your specific requirements\n",
    "        rotated_image = np.concatenate((image[angle:], image[:angle]))\n",
    "        rotated_data.append(rotated_image)\n",
    "    return np.array(rotated_data)\n",
    "\n",
    "# Add Noise Function\n",
    "def add_noise(data, mean, std_dev):\n",
    "    noisy_data = data + np.random.normal(mean, std_dev, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Rotate the data\n",
    "X_train = rotate_data(X_train, angle=10)\n",
    "X_test = rotate_data(X_test, angle=10)\n",
    "\n",
    "\n",
    "# Add noise to the data\n",
    "X_train = add_noise(X_train, 0, 0.1)\n",
    "X_test = add_noise(X_test, 0, 0.1)\n",
    "\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.025  # Adjust as necessary\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the NN architecture\n",
    "# Define the NN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(64, input_shape=INPUT_SHAPE),\n",
    "#     tf.keras.layers.LeakyReLU(alpha=0.2),  # Replace activation with LeakyReLU\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Dense(128),\n",
    "#     tf.keras.layers.ELU(alpha=0.2),  # Replace activation with LeakyReLU\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(256),\n",
    "#     tf.keras.layers.LeakyReLU(alpha=0.2),  # Replace activation with LeakyReLU\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(128),\n",
    "#     tf.keras.layers.LeakyReLU(alpha=0.2),  # Replace activation with LeakyReLU\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(64),\n",
    "#     tf.keras.layers.LeakyReLU(alpha=0.2),  # Replace activation with LeakyReLU\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "# ])\n",
    "\n",
    "\n",
    "# Define an Adam optimizer with the desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs=30,\n",
    "                    batch_size=64)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_test_pred_prob = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Model accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_462 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_385 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_463 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_386 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_308 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_231 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_464 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_387 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_309 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_232 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_465 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_388 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_310 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_233 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_466 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_389 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_311 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_467 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 2s 7ms/step - loss: 2.3802 - accuracy: 0.1267\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 2.1917 - accuracy: 0.1726\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 2.0714 - accuracy: 0.2400\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.9801 - accuracy: 0.2817\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.9101 - accuracy: 0.3094\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.8471 - accuracy: 0.3514\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.7785 - accuracy: 0.3840\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.7406 - accuracy: 0.4076\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.6765 - accuracy: 0.4243\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.6674 - accuracy: 0.4291\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.6302 - accuracy: 0.4437\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.5934 - accuracy: 0.4626\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5925 - accuracy: 0.4700\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.5455 - accuracy: 0.4866\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5005 - accuracy: 0.4986\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.5034 - accuracy: 0.5004\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4890 - accuracy: 0.5074\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4613 - accuracy: 0.5140\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4834 - accuracy: 0.5119\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4204 - accuracy: 0.5266\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4550 - accuracy: 0.5203\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3988 - accuracy: 0.5423\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4082 - accuracy: 0.5346\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3923 - accuracy: 0.5389\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3556 - accuracy: 0.5580\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3717 - accuracy: 0.5491\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3388 - accuracy: 0.5577\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3349 - accuracy: 0.5607\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3226 - accuracy: 0.5717\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.2994 - accuracy: 0.5780\n",
      "Model accuracy: 0.545\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_468 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_390 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_469 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_391 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_312 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_234 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_470 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_392 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_313 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_235 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_471 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_393 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_314 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_236 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_472 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_394 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_315 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_473 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 2s 6ms/step - loss: 2.3756 - accuracy: 0.1363\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 2.1572 - accuracy: 0.1866\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 2.0717 - accuracy: 0.2354\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.9766 - accuracy: 0.2830\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.9144 - accuracy: 0.3087\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.8463 - accuracy: 0.3503\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.7745 - accuracy: 0.3819\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.7008 - accuracy: 0.4163\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.6659 - accuracy: 0.4239\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.6438 - accuracy: 0.4439\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.6105 - accuracy: 0.4560\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5744 - accuracy: 0.4776\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.5446 - accuracy: 0.4803\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5274 - accuracy: 0.4804\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4962 - accuracy: 0.5070\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4645 - accuracy: 0.5121\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4446 - accuracy: 0.5231\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4490 - accuracy: 0.5304\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3979 - accuracy: 0.5439\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3970 - accuracy: 0.5393\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3861 - accuracy: 0.5483\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3688 - accuracy: 0.5507\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3642 - accuracy: 0.5556\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3438 - accuracy: 0.5636\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3261 - accuracy: 0.5720\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3202 - accuracy: 0.5727\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3051 - accuracy: 0.5746\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.2984 - accuracy: 0.5821\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.2954 - accuracy: 0.5810\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.2831 - accuracy: 0.5893\n",
      "Model accuracy: 0.545\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_474 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_395 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_475 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_396 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_316 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_237 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_476 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_397 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_317 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_238 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_477 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_398 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_318 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_239 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_478 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_399 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_319 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_479 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 2s 6ms/step - loss: 2.3991 - accuracy: 0.1180\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 2.2199 - accuracy: 0.1553\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 2.1191 - accuracy: 0.2170\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 2.0018 - accuracy: 0.2741\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.9138 - accuracy: 0.3199\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.8426 - accuracy: 0.3507\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.7791 - accuracy: 0.3881\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.7164 - accuracy: 0.4164\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.6896 - accuracy: 0.4254\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.6491 - accuracy: 0.4483\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.6298 - accuracy: 0.4573\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.5914 - accuracy: 0.4680\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.5524 - accuracy: 0.4847\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.5386 - accuracy: 0.4947\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.5149 - accuracy: 0.4967\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4938 - accuracy: 0.5043\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4806 - accuracy: 0.5147\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4726 - accuracy: 0.5097\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4445 - accuracy: 0.5224\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4431 - accuracy: 0.5200\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4085 - accuracy: 0.5406\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4138 - accuracy: 0.5324\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.4007 - accuracy: 0.5377\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3847 - accuracy: 0.5529\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3746 - accuracy: 0.5494\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3641 - accuracy: 0.5486\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3513 - accuracy: 0.5643\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3317 - accuracy: 0.5670\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3115 - accuracy: 0.5734\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.3048 - accuracy: 0.5723\n",
      "Model accuracy: 0.545\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_480 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_400 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_481 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_401 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_320 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_240 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_482 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_402 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_321 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_241 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_483 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_403 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_322 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_242 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_484 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_404 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_323 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_485 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 3s 8ms/step - loss: 2.3742 - accuracy: 0.1284\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 2.1296 - accuracy: 0.2114\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 2.0378 - accuracy: 0.2616\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.9598 - accuracy: 0.2961\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.8796 - accuracy: 0.3364\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.8207 - accuracy: 0.3711\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.7619 - accuracy: 0.4000\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.7147 - accuracy: 0.4096\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.6702 - accuracy: 0.4371\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.6329 - accuracy: 0.4509\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.5828 - accuracy: 0.4704\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.5642 - accuracy: 0.4709\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.5357 - accuracy: 0.4819\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.5162 - accuracy: 0.4890\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.4955 - accuracy: 0.5063\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.4726 - accuracy: 0.5107\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.4602 - accuracy: 0.5190\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.4410 - accuracy: 0.5264\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.4135 - accuracy: 0.5356\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3976 - accuracy: 0.5393\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3822 - accuracy: 0.5420\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.3531 - accuracy: 0.5527\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.3576 - accuracy: 0.5547\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.3532 - accuracy: 0.5591\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.3245 - accuracy: 0.5729\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.3383 - accuracy: 0.5620\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 1.3190 - accuracy: 0.5633\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 1.3014 - accuracy: 0.5754\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.2902 - accuracy: 0.5724\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 1.2840 - accuracy: 0.5787\n",
      "Model accuracy: 0.545\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_486 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_405 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_487 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_406 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_324 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_243 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_488 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_407 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_325 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_244 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_489 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_408 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_326 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_245 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_490 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_409 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_327 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_491 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 3s 10ms/step - loss: 2.3555 - accuracy: 0.1359\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 2.1266 - accuracy: 0.2074\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 2.0148 - accuracy: 0.2641\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.9376 - accuracy: 0.3073\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.8625 - accuracy: 0.3361\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.7897 - accuracy: 0.3721\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.7493 - accuracy: 0.4011\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.6810 - accuracy: 0.4304\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.6601 - accuracy: 0.4433\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.6314 - accuracy: 0.4487\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.5739 - accuracy: 0.4727\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.5714 - accuracy: 0.4756\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.5375 - accuracy: 0.4873\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.5239 - accuracy: 0.4910\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.4905 - accuracy: 0.5026\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.4871 - accuracy: 0.5091\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.4447 - accuracy: 0.5221\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.4288 - accuracy: 0.5243\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.4336 - accuracy: 0.5284\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.4150 - accuracy: 0.5327\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3800 - accuracy: 0.5381\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3811 - accuracy: 0.5477\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3574 - accuracy: 0.5586\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3452 - accuracy: 0.5626\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3399 - accuracy: 0.5664\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3224 - accuracy: 0.5657\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3228 - accuracy: 0.5687\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3033 - accuracy: 0.5821\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3003 - accuracy: 0.5779\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.2907 - accuracy: 0.5781\n",
      "Model accuracy: 0.545\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_492 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_410 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_493 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_411 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_328 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_246 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_494 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_412 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_329 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_247 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_495 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_413 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_330 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_248 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_496 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_414 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_331 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_497 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 3s 9ms/step - loss: 2.3839 - accuracy: 0.1153\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 2.1800 - accuracy: 0.1831\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 2.0619 - accuracy: 0.2339\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.9811 - accuracy: 0.2800\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.9124 - accuracy: 0.3260\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.8060 - accuracy: 0.3766\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.7456 - accuracy: 0.3991\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.6961 - accuracy: 0.4194\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.6563 - accuracy: 0.4341\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.6037 - accuracy: 0.4603\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5756 - accuracy: 0.4720\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5578 - accuracy: 0.4776\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5456 - accuracy: 0.4879\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5128 - accuracy: 0.4909\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4607 - accuracy: 0.5146\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4497 - accuracy: 0.5254\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4415 - accuracy: 0.5183\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4046 - accuracy: 0.5433\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4279 - accuracy: 0.5344\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3733 - accuracy: 0.5517\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3889 - accuracy: 0.5436\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3458 - accuracy: 0.5630\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3730 - accuracy: 0.5513\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3260 - accuracy: 0.5646\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3270 - accuracy: 0.5664\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.2961 - accuracy: 0.5834\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.2915 - accuracy: 0.5836\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.2720 - accuracy: 0.5897\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.2752 - accuracy: 0.5814\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.2712 - accuracy: 0.5886\n",
      "Model accuracy: 0.545\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_498 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_415 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_499 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_416 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_332 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_249 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_500 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_417 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_333 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_250 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_501 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_418 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_334 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_251 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_502 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_419 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_335 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_503 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 2s 7ms/step - loss: 2.3696 - accuracy: 0.1301\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 2.1685 - accuracy: 0.1900\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 2.0559 - accuracy: 0.2424\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.9766 - accuracy: 0.2880\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.8982 - accuracy: 0.3244\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.8409 - accuracy: 0.3571\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.7544 - accuracy: 0.3900\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.7101 - accuracy: 0.4073\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.6685 - accuracy: 0.4337\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.6371 - accuracy: 0.4494\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.6079 - accuracy: 0.4560\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5757 - accuracy: 0.4690\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5614 - accuracy: 0.4739\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5203 - accuracy: 0.4926\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.5070 - accuracy: 0.5031\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4705 - accuracy: 0.5074\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4535 - accuracy: 0.5140\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4287 - accuracy: 0.5299\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4287 - accuracy: 0.5339\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4085 - accuracy: 0.5379\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.4051 - accuracy: 0.5419\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3747 - accuracy: 0.5459\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3459 - accuracy: 0.5623\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3359 - accuracy: 0.5691\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3313 - accuracy: 0.5654\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.3351 - accuracy: 0.5614\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.3263 - accuracy: 0.5737\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.2961 - accuracy: 0.5823\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.2872 - accuracy: 0.5806\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.2678 - accuracy: 0.5856\n",
      "Model accuracy: 0.545\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_504 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_420 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_505 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_421 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_336 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_252 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_506 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_422 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_337 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_253 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_507 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_423 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_338 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_254 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_508 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_424 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_339 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_509 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "# Create an ensemble of neural networks\n",
    "ensemble_size = 10\n",
    "ensemble = []\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.025  # Adjust as necessary\n",
    "\n",
    "# Train individual neural network models\n",
    "for _ in range(ensemble_size):\n",
    "    model = tf.keras.Sequential([\n",
    "         tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model accuracy:', accuracy)\n",
    "    \n",
    "    ensemble.append(model)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred_prob = np.zeros((len(y_test), np.unique(y_train).shape[0]))\n",
    "for model in ensemble:\n",
    "    y_test_pred_prob += model.predict(X_test)\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Ensemble accuracy:', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
