{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Increase the complexity of the CNN and Ensemble methods\n",
    "#### Ensemble methods: Instead of training a single model, you can try using ensemble methods such as bagging or boosting. These techniques involve training multiple models and combining their predictions, which often leads to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model 1 Training:\n",
      "Epoch 1/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.2535 - accuracy: 0.1460\n",
      "Epoch 2/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.1251 - accuracy: 0.2115\n",
      "Epoch 3/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 2.0086 - accuracy: 0.2608\n",
      "Epoch 4/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.8951 - accuracy: 0.3075\n",
      "Epoch 5/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.7780 - accuracy: 0.3473\n",
      "Epoch 6/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.6656 - accuracy: 0.3890\n",
      "Epoch 7/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.5670 - accuracy: 0.4295\n",
      "Epoch 8/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.4826 - accuracy: 0.4550\n",
      "Epoch 9/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 1.4041 - accuracy: 0.4852\n",
      "Epoch 10/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.3221 - accuracy: 0.5144\n",
      "Epoch 11/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.2557 - accuracy: 0.5405\n",
      "Epoch 12/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.2123 - accuracy: 0.5535\n",
      "Epoch 13/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.1565 - accuracy: 0.5718\n",
      "Epoch 14/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.1232 - accuracy: 0.5832\n",
      "Epoch 15/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.0797 - accuracy: 0.6009\n",
      "Epoch 16/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.0544 - accuracy: 0.6100\n",
      "Epoch 17/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.0223 - accuracy: 0.6189\n",
      "Epoch 18/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.0105 - accuracy: 0.6225\n",
      "Epoch 19/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.9734 - accuracy: 0.6376\n",
      "Epoch 20/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.9470 - accuracy: 0.6454\n",
      "Epoch 21/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.9356 - accuracy: 0.6505\n",
      "Epoch 22/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.9267 - accuracy: 0.6517\n",
      "Epoch 23/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.9051 - accuracy: 0.6621\n",
      "Epoch 24/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.8738 - accuracy: 0.6761\n",
      "Epoch 25/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8727 - accuracy: 0.6717\n",
      "Epoch 26/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8565 - accuracy: 0.6799\n",
      "Epoch 27/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8423 - accuracy: 0.6829\n",
      "Epoch 28/30\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.8322 - accuracy: 0.6878\n",
      "Epoch 29/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8326 - accuracy: 0.6880\n",
      "Epoch 30/30\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.8053 - accuracy: 0.6997\n",
      "94/94 [==============================] - 0s 964us/step\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_77 (Conv1D)          (None, 69, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_77 (MaxPoolin  (None, 34, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_78 (Conv1D)          (None, 32, 64)            6208      \n",
      "                                                                 \n",
      " max_pooling1d_78 (MaxPoolin  (None, 16, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_36 (Flatten)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 32)                32800     \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,466\n",
      "Trainable params: 39,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Accuracy: 0.25666666666666665\n",
      "Model Loss: 0.8053159117698669\n",
      "\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "Ensemble Model Accuracy: 0.25666666666666665\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the training data and labels\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Assuming X_train is a Pandas Series\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "X_train = X_train.values.reshape(-1, 71, 1)\n",
    "X_test = X_test.values.reshape(-1, 71, 1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71, 1)  # Number of input features and channels\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.001  # Adjust as necessary\n",
    "NUM_MODELS = 1  # Number of models in the ensemble\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the ensemble of models\n",
    "ensemble_models = []\n",
    "for _ in range(NUM_MODELS):\n",
    "    # Define the CNN architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Compile the model with the custom optimizer\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    # Append the model to the ensemble\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "# Train each model in the ensemble\n",
    "for model_index, model in enumerate(ensemble_models):\n",
    "    print(f\"Model {model_index + 1} Training:\")\n",
    "    history = model.fit(X_train, y_train_encoded,\n",
    "                        epochs=30,\n",
    "                        batch_size=32,\n",
    "                        verbose=1)  # Set verbose=1 to see training progress\n",
    "\n",
    "    # Predicting the test set results\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model Accuracy:', accuracy)\n",
    "    print('Model Loss:', history.history['loss'][-1])\n",
    "    print()\n",
    "\n",
    "# Predicting the test set results using the ensemble\n",
    "y_test_preds = []\n",
    "for model in ensemble_models:\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "# Take the majority vote from the ensemble predictions\n",
    "y_test_preds_ensemble = np.round(np.mean(y_test_preds, axis=0)).astype(int)\n",
    "\n",
    "# Calculate accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, y_test_preds_ensemble)\n",
    "print('Ensemble Model Accuracy:', ensemble_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_123 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,754\n",
      "Trainable params: 87,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "219/219 [==============================] - 1s 1ms/step - loss: 0.0899 - accuracy: 0.1102\n",
      "Epoch 2/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.1507\n",
      "Epoch 3/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.1647\n",
      "Epoch 4/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.1961\n",
      "Epoch 5/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.2219\n",
      "Epoch 6/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.2389\n",
      "Epoch 7/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.2664\n",
      "Epoch 8/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.2889\n",
      "Epoch 9/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3059\n",
      "Epoch 10/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.3209\n",
      "Epoch 11/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3377\n",
      "Epoch 12/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.3515\n",
      "Epoch 13/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3556\n",
      "Epoch 14/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.3702\n",
      "Epoch 15/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.3724\n",
      "Epoch 16/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.3807\n",
      "Epoch 17/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3946\n",
      "Epoch 18/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3979\n",
      "Epoch 19/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.4145\n",
      "Epoch 20/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.4174\n",
      "Epoch 21/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.4209\n",
      "Epoch 22/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.4283\n",
      "Epoch 23/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.4410\n",
      "Epoch 24/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.4474\n",
      "Epoch 25/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.4606\n",
      "Epoch 26/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.4649\n",
      "Epoch 27/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.4724\n",
      "Epoch 28/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.4794\n",
      "Epoch 29/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.4894\n",
      "Epoch 30/30\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.4928\n",
      "94/94 [==============================] - 0s 706us/step\n",
      "Model accuracy: 0.3516666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rotate Data Function\n",
    "def rotate_data(data, angle):\n",
    "    rotated_data = []\n",
    "    for image in data:\n",
    "        rotated_image = np.concatenate((image[angle:], image[:angle]))\n",
    "        rotated_data.append(rotated_image)\n",
    "    return np.array(rotated_data)\n",
    "\n",
    "# Add Noise Function\n",
    "def add_noise(data, mean, std_dev):\n",
    "    noisy_data = data + np.random.normal(mean, std_dev, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Normalize the input data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train[i]\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to NumPy arrays\n",
    "X_train = np.array(augmented_X_train)\n",
    "y_train = np.array(augmented_y_train)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Rotate the data\n",
    "X_train = rotate_data(X_train, angle=10)\n",
    "X_test = rotate_data(X_test, angle=10)\n",
    "\n",
    "# Add noise to the data\n",
    "X_train = add_noise(X_train, 0, 0.1)\n",
    "X_test = add_noise(X_test, 0, 0.1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.001  # Adjust as necessary\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the NN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define an Adam optimizer with the desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs=30,\n",
    "                    batch_size=64)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_test_pred_prob = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Model accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_602 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_502 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_603 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_503 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_402 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_303 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_604 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_504 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_403 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_304 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_605 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_505 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_404 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_305 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_606 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_506 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_405 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_607 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "165/165 [==============================] - 3s 11ms/step - loss: 0.0903 - accuracy: 0.1444\n",
      "Epoch 2/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0835 - accuracy: 0.2572\n",
      "Epoch 3/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0793 - accuracy: 0.3255\n",
      "Epoch 4/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0763 - accuracy: 0.3731\n",
      "Epoch 5/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0730 - accuracy: 0.4127\n",
      "Epoch 6/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0706 - accuracy: 0.4398\n",
      "Epoch 7/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0692 - accuracy: 0.4560\n",
      "Epoch 8/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0670 - accuracy: 0.4821\n",
      "Epoch 9/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0661 - accuracy: 0.4900\n",
      "Epoch 10/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0643 - accuracy: 0.5130\n",
      "Epoch 11/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0638 - accuracy: 0.5152\n",
      "Epoch 12/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0624 - accuracy: 0.5312\n",
      "Epoch 13/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0614 - accuracy: 0.5416\n",
      "Epoch 14/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0610 - accuracy: 0.5449\n",
      "Epoch 15/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0598 - accuracy: 0.5563\n",
      "Epoch 16/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0586 - accuracy: 0.5705\n",
      "Epoch 17/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0581 - accuracy: 0.5728\n",
      "Epoch 18/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0581 - accuracy: 0.5749\n",
      "Epoch 19/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0575 - accuracy: 0.5793\n",
      "Epoch 20/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0574 - accuracy: 0.5807\n",
      "Epoch 21/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0560 - accuracy: 0.5915\n",
      "Epoch 22/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0560 - accuracy: 0.5929\n",
      "Epoch 23/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0552 - accuracy: 0.6031\n",
      "Epoch 24/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0550 - accuracy: 0.6023\n",
      "Epoch 25/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0548 - accuracy: 0.6047\n",
      "Epoch 26/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0540 - accuracy: 0.6111\n",
      "Epoch 27/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0532 - accuracy: 0.6183\n",
      "Epoch 28/30\n",
      "165/165 [==============================] - 2s 11ms/step - loss: 0.0523 - accuracy: 0.6267\n",
      "Epoch 29/30\n",
      "165/165 [==============================] - 2s 12ms/step - loss: 0.0534 - accuracy: 0.6150\n",
      "Epoch 30/30\n",
      "165/165 [==============================] - 2s 12ms/step - loss: 0.0517 - accuracy: 0.6324\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.538\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Rotate Data Function\n",
    "def rotate_data(data, angle):\n",
    "    rotated_data = []\n",
    "    for _, row in data.iterrows():\n",
    "        image = row.to_numpy().reshape((71,))\n",
    "        # Perform rotation operation specific to your data shape\n",
    "        # Modify the rotation operation based on your specific requirements\n",
    "        rotated_image = np.concatenate((image[angle:], image[:angle]))\n",
    "        rotated_data.append(rotated_image)\n",
    "    return np.array(rotated_data)\n",
    "\n",
    "# Add Noise Function\n",
    "def add_noise(data, mean, std_dev):\n",
    "    noisy_data = data + np.random.normal(mean, std_dev, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Rotate the data\n",
    "X_train = rotate_data(X_train, angle=10)\n",
    "X_test = rotate_data(X_test, angle=10)\n",
    "\n",
    "\n",
    "# Add noise to the data\n",
    "X_train = add_noise(X_train, 0, 0.1)\n",
    "X_test = add_noise(X_test, 0, 0.1)\n",
    "\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.025  # Adjust as necessary\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the NN architecture\n",
    "# Define the NN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define an Adam optimizer with the desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs=30,\n",
    "                    batch_size=128)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_test_pred_prob = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Model accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m X_test \u001b[39m=\u001b[39m add_noise(X_test, \u001b[39m0\u001b[39m, \u001b[39m0.1\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[39m# Convert the data to numpy arrays\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39;49mto_numpy()\n\u001b[0;32m     73\u001b[0m X_test \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m     74\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mravel(y_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create an ensemble of neural networks\n",
    "ensemble_size = 10\n",
    "ensemble = []\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.025  # Adjust as necessary\n",
    "\n",
    "# Train individual neural network models\n",
    "for _ in range(ensemble_size):\n",
    "    model = tf.keras.Sequential([\n",
    "         tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model accuracy:', accuracy)\n",
    "    \n",
    "    ensemble.append(model)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred_prob = np.zeros((len(y_test), np.unique(y_train).shape[0]))\n",
    "for model in ensemble:\n",
    "    y_test_pred_prob += model.predict(X_test)\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Ensemble accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_528 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_440 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_529 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_441 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_352 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_264 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_530 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_442 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_353 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_265 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_531 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_443 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_354 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_266 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_532 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_444 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_355 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_533 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 4s 7ms/step - loss: 2.2060 - accuracy: 0.1810\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 1.9033 - accuracy: 0.3230\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.7532 - accuracy: 0.3972\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.6575 - accuracy: 0.4392\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.5812 - accuracy: 0.4757\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 1.5267 - accuracy: 0.4957\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 1.4736 - accuracy: 0.5127\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.4311 - accuracy: 0.5281\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.4113 - accuracy: 0.5353\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.3652 - accuracy: 0.5517\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.3392 - accuracy: 0.5630\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.3039 - accuracy: 0.5771\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.2966 - accuracy: 0.5770\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.2826 - accuracy: 0.5800\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.2510 - accuracy: 0.5940\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.2421 - accuracy: 0.5963\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.2198 - accuracy: 0.6057\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1983 - accuracy: 0.6110\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1876 - accuracy: 0.6150\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1693 - accuracy: 0.6219\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 1.1675 - accuracy: 0.6229\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1524 - accuracy: 0.6289\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1479 - accuracy: 0.6304\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1353 - accuracy: 0.6348\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1182 - accuracy: 0.6410\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1204 - accuracy: 0.6379\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1132 - accuracy: 0.6441\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.1083 - accuracy: 0.6424\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.0933 - accuracy: 0.6458\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 1.0752 - accuracy: 0.6568\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.5346666666666666\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_534 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_445 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_535 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_446 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_356 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_267 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_536 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_447 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_357 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_268 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_537 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_448 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_358 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_269 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_538 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_449 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_359 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_539 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 5s 8ms/step - loss: 2.2462 - accuracy: 0.1590\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.9732 - accuracy: 0.2848\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.8078 - accuracy: 0.3689\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6979 - accuracy: 0.4200\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6005 - accuracy: 0.4638\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5335 - accuracy: 0.4903\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5087 - accuracy: 0.5002\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4435 - accuracy: 0.5262\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4195 - accuracy: 0.5328\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3853 - accuracy: 0.5432\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3433 - accuracy: 0.5598\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3456 - accuracy: 0.5606\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3199 - accuracy: 0.5650\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2805 - accuracy: 0.5791\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2803 - accuracy: 0.5798\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2608 - accuracy: 0.5902\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2422 - accuracy: 0.5915\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2201 - accuracy: 0.6042\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2060 - accuracy: 0.6047\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1989 - accuracy: 0.6130\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1731 - accuracy: 0.6204\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1768 - accuracy: 0.6147\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1715 - accuracy: 0.6200\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1524 - accuracy: 0.6271\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1306 - accuracy: 0.6282\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1453 - accuracy: 0.6265\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1239 - accuracy: 0.6378\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1022 - accuracy: 0.6424\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0986 - accuracy: 0.6431\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1059 - accuracy: 0.6379\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.546\n",
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_540 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_450 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_541 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_451 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_360 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_270 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_542 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_452 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_361 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_271 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_543 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_453 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_362 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_272 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_544 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_454 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_363 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_545 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 4s 8ms/step - loss: 2.2032 - accuracy: 0.1856\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.8997 - accuracy: 0.3242\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.7774 - accuracy: 0.3838\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6651 - accuracy: 0.4325\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5762 - accuracy: 0.4711\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5174 - accuracy: 0.4921\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4789 - accuracy: 0.5117\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4211 - accuracy: 0.5350\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3771 - accuracy: 0.5461\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3483 - accuracy: 0.5561\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3103 - accuracy: 0.5735\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.3027 - accuracy: 0.5776\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2824 - accuracy: 0.5801\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2524 - accuracy: 0.5936\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2244 - accuracy: 0.6051\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2154 - accuracy: 0.6107\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.2318 - accuracy: 0.6023\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.1930 - accuracy: 0.6150\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1762 - accuracy: 0.6212\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1899 - accuracy: 0.6165\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1455 - accuracy: 0.6315\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1803 - accuracy: 0.6168\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1328 - accuracy: 0.6352\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1299 - accuracy: 0.6358\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1167 - accuracy: 0.6427\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1079 - accuracy: 0.6460\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0979 - accuracy: 0.6490\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0843 - accuracy: 0.6519\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0823 - accuracy: 0.6526\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0699 - accuracy: 0.6579\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.5483333333333333\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_546 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_455 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_547 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_456 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_364 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_273 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_548 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_457 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_365 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_274 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_549 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_458 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_366 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_275 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_550 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_459 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_367 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_551 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 4s 8ms/step - loss: 2.1972 - accuracy: 0.1873\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.8900 - accuracy: 0.3351\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.7393 - accuracy: 0.4006\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.6348 - accuracy: 0.4491\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5663 - accuracy: 0.4752\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.5316 - accuracy: 0.4877\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.4763 - accuracy: 0.5131\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.4412 - accuracy: 0.5265\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.4358 - accuracy: 0.5268\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.3849 - accuracy: 0.5499\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.3540 - accuracy: 0.5565\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3404 - accuracy: 0.5600\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3174 - accuracy: 0.5686\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.2892 - accuracy: 0.5832\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.2828 - accuracy: 0.5872\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.2721 - accuracy: 0.5851\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.2511 - accuracy: 0.5959\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2446 - accuracy: 0.6000\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.2244 - accuracy: 0.6049\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.2188 - accuracy: 0.6058\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2069 - accuracy: 0.6108\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1837 - accuracy: 0.6200\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1727 - accuracy: 0.6202\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1608 - accuracy: 0.6261\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.1577 - accuracy: 0.6237\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.1506 - accuracy: 0.6249\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.1436 - accuracy: 0.6290\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.1445 - accuracy: 0.6317\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.1383 - accuracy: 0.6285\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1233 - accuracy: 0.6383\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.542\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_552 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_460 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_553 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_461 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_368 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_276 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_554 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_462 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_369 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_277 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_555 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_463 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_370 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_278 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_556 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_464 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_371 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_557 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 5s 8ms/step - loss: 2.1832 - accuracy: 0.1931\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.8868 - accuracy: 0.3339\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.7253 - accuracy: 0.4080\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6280 - accuracy: 0.4510\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5380 - accuracy: 0.4902\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4975 - accuracy: 0.5038\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4358 - accuracy: 0.5250\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3971 - accuracy: 0.5403\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3695 - accuracy: 0.5479\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3412 - accuracy: 0.5649\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3204 - accuracy: 0.5740\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2853 - accuracy: 0.5810\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2814 - accuracy: 0.5822\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2576 - accuracy: 0.5920\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2289 - accuracy: 0.6001\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2169 - accuracy: 0.6051\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2029 - accuracy: 0.6109\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1879 - accuracy: 0.6160\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1676 - accuracy: 0.6220\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1569 - accuracy: 0.6292\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1425 - accuracy: 0.6301\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1396 - accuracy: 0.6345\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1376 - accuracy: 0.6348\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1233 - accuracy: 0.6383\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1048 - accuracy: 0.6454\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0880 - accuracy: 0.6524\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0876 - accuracy: 0.6513\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0746 - accuracy: 0.6552\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0651 - accuracy: 0.6579\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0610 - accuracy: 0.6574\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.5593333333333333\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_558 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_465 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_559 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_466 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_372 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_279 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_560 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_467 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_373 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_280 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_561 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_468 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_374 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_281 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_562 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_469 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_375 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_563 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 5s 8ms/step - loss: 2.2044 - accuracy: 0.1834\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.9011 - accuracy: 0.3233\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.7429 - accuracy: 0.4000\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6563 - accuracy: 0.4385\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5914 - accuracy: 0.4680\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5448 - accuracy: 0.4809\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4892 - accuracy: 0.5084\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4523 - accuracy: 0.5237\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4163 - accuracy: 0.5319\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3813 - accuracy: 0.5463\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3605 - accuracy: 0.5547\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3428 - accuracy: 0.5630\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3089 - accuracy: 0.5712\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3018 - accuracy: 0.5757\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2595 - accuracy: 0.5904\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2486 - accuracy: 0.5951\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2381 - accuracy: 0.5994\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2210 - accuracy: 0.6064\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2059 - accuracy: 0.6086\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1940 - accuracy: 0.6152\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1703 - accuracy: 0.6239\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1778 - accuracy: 0.6229\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1526 - accuracy: 0.6292\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1531 - accuracy: 0.6255\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1439 - accuracy: 0.6308\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1150 - accuracy: 0.6405\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1175 - accuracy: 0.6384\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1141 - accuracy: 0.6430\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1082 - accuracy: 0.6410\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0971 - accuracy: 0.6464\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.535\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_564 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_470 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_565 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_471 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_376 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_282 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_566 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_472 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_377 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_283 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_567 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_473 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_378 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_284 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_568 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_474 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_379 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_569 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 5s 8ms/step - loss: 2.2323 - accuracy: 0.1720\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.9197 - accuracy: 0.3117\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.7617 - accuracy: 0.3910\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6669 - accuracy: 0.4323\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5992 - accuracy: 0.4563\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5406 - accuracy: 0.4856\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4975 - accuracy: 0.4997\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4727 - accuracy: 0.5086\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4277 - accuracy: 0.5240\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4010 - accuracy: 0.5379\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3682 - accuracy: 0.5466\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3408 - accuracy: 0.5562\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3057 - accuracy: 0.5721\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2818 - accuracy: 0.5780\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2720 - accuracy: 0.5836\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2679 - accuracy: 0.5888\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2220 - accuracy: 0.6002\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2151 - accuracy: 0.6100\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1927 - accuracy: 0.6138\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1938 - accuracy: 0.6131\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1901 - accuracy: 0.6138\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1671 - accuracy: 0.6177\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1530 - accuracy: 0.6252\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1864 - accuracy: 0.6115\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1711 - accuracy: 0.6210\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1506 - accuracy: 0.6271\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1379 - accuracy: 0.6365\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1154 - accuracy: 0.6381\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1119 - accuracy: 0.6452\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1011 - accuracy: 0.6453\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.554\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_570 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_475 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_571 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_476 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_380 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_285 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_572 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_477 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_381 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_286 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_573 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_478 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_382 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_287 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_574 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_479 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_383 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_575 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 5s 8ms/step - loss: 2.2057 - accuracy: 0.1902\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.9057 - accuracy: 0.3186\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 10ms/step - loss: 1.7416 - accuracy: 0.3923\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6402 - accuracy: 0.4421\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5688 - accuracy: 0.4691\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5154 - accuracy: 0.4909\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 2s 7ms/step - loss: 1.4583 - accuracy: 0.5130\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4125 - accuracy: 0.5340\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3920 - accuracy: 0.5400\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3526 - accuracy: 0.5570\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3350 - accuracy: 0.5645\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3061 - accuracy: 0.5748\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2811 - accuracy: 0.5820\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2655 - accuracy: 0.5836\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2399 - accuracy: 0.5992\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2092 - accuracy: 0.6107\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2057 - accuracy: 0.6096\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1814 - accuracy: 0.6173\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1749 - accuracy: 0.6189\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1648 - accuracy: 0.6230\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1532 - accuracy: 0.6262\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1325 - accuracy: 0.6322\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1501 - accuracy: 0.6278\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1146 - accuracy: 0.6380\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1319 - accuracy: 0.6338\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1017 - accuracy: 0.6427\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0830 - accuracy: 0.6501\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0787 - accuracy: 0.6510\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0656 - accuracy: 0.6562\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0652 - accuracy: 0.6546\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.5493333333333333\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_576 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_480 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_577 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_481 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_384 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_288 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_578 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_482 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_385 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_289 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_579 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_483 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_386 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_290 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_580 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_484 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_387 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_581 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 5s 8ms/step - loss: 2.1866 - accuracy: 0.1953\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.8994 - accuracy: 0.3205\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.7339 - accuracy: 0.4032\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6446 - accuracy: 0.4432\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5666 - accuracy: 0.4764\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5206 - accuracy: 0.4915\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4847 - accuracy: 0.5121\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4513 - accuracy: 0.5240\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4100 - accuracy: 0.5363\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3718 - accuracy: 0.5525\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3432 - accuracy: 0.5599\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3190 - accuracy: 0.5679\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2965 - accuracy: 0.5768\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2713 - accuracy: 0.5832\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6391 - accuracy: 0.4458\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4293 - accuracy: 0.5294\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3682 - accuracy: 0.5540\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3191 - accuracy: 0.5706\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2921 - accuracy: 0.5797\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2666 - accuracy: 0.5896\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2584 - accuracy: 0.5886\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2409 - accuracy: 0.5972\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2285 - accuracy: 0.6033\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2213 - accuracy: 0.6033\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1938 - accuracy: 0.6137\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1818 - accuracy: 0.6141\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1691 - accuracy: 0.6174\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1727 - accuracy: 0.6182\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1482 - accuracy: 0.6256\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1373 - accuracy: 0.6281\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.5513333333333333\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_582 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " batch_normalization_485 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_583 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_486 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_388 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_291 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_584 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_487 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_389 (Flatten)       (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_292 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_585 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_488 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_390 (Flatten)       (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_293 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_586 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_489 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_391 (Flatten)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_587 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,314\n",
      "Trainable params: 89,034\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 5s 8ms/step - loss: 2.1912 - accuracy: 0.1893\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.9172 - accuracy: 0.3092\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.7565 - accuracy: 0.3918\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.6457 - accuracy: 0.4421\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5810 - accuracy: 0.4700\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.5118 - accuracy: 0.4969\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4476 - accuracy: 0.5215\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.4346 - accuracy: 0.5221\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3868 - accuracy: 0.5450\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3633 - accuracy: 0.5551\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3471 - accuracy: 0.5612\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.3095 - accuracy: 0.5744\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2860 - accuracy: 0.5838\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2644 - accuracy: 0.5902\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2434 - accuracy: 0.5967\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2303 - accuracy: 0.6017\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.2089 - accuracy: 0.6113\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1879 - accuracy: 0.6210\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1953 - accuracy: 0.6145\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1704 - accuracy: 0.6234\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1631 - accuracy: 0.6256\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1467 - accuracy: 0.6266\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1340 - accuracy: 0.6365\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.1142 - accuracy: 0.6414\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.1100 - accuracy: 0.6428\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 3s 9ms/step - loss: 1.1059 - accuracy: 0.6466\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0867 - accuracy: 0.6497\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0809 - accuracy: 0.6543\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0697 - accuracy: 0.6576\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 3s 8ms/step - loss: 1.0831 - accuracy: 0.6532\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.552\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Ensemble accuracy: 0.5963333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Rotate Data Function\n",
    "def rotate_data(data, angle):\n",
    "    rotated_data = np.concatenate((data[:, angle:], data[:, :angle]), axis=1)\n",
    "    return rotated_data\n",
    "\n",
    "# Add Noise Function\n",
    "def add_noise(data, mean, std_dev):\n",
    "    noisy_data = data + np.random.normal(mean, std_dev, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train[i]\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train[i])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train[i])\n",
    "\n",
    "# Convert augmented data to numpy arrays\n",
    "augmented_X_train = np.array(augmented_X_train)\n",
    "augmented_y_train = np.array(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = np.concatenate([X_train, augmented_X_train], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Rotate the data\n",
    "X_train = rotate_data(X_train, angle=10)\n",
    "X_test = rotate_data(X_test, angle=10)\n",
    "\n",
    "# Add noise to the data\n",
    "X_train = add_noise(X_train, 0, 0.1)\n",
    "X_test = add_noise(X_test, 0, 0.1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.025  # Adjust as necessary\n",
    "\n",
    "# Create an ensemble of neural networks\n",
    "ensemble_size = 10\n",
    "ensemble = []\n",
    "\n",
    "# Train individual neural network models\n",
    "for _ in range(ensemble_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Additional hidden layer\n",
    "        tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model accuracy:', accuracy)\n",
    "\n",
    "    ensemble.append(model)\n",
    "\n",
    "# Make predictions on the test set using the ensemble\n",
    "y_test_pred_prob = np.zeros((len(y_test), NUM_CLASSES))\n",
    "for model in ensemble:\n",
    "    y_test_pred_prob += model.predict(X_test)\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy of the ensemble\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Ensemble accuracy:', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
