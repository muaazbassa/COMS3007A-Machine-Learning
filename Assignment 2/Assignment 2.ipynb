{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.334375\n",
      "Unseen Data Accuracy: 0.3795\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data\n",
    "val_predictions = classifier.predict(val_data)\n",
    "unseen_predictions = classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 50, 'n_estimators': 500}\n",
      "Validation Accuracy: 0.40265625\n",
      "Validation Accuracy: 0.42375\n",
      "Unseen Data Accuracy: 0.4485\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [5, 10, 20, None]\n",
    "# }\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 100, 'n_estimators': 1000}\n",
      "Validation Accuracy: 0.41734375\n",
      "Validation Accuracy: 0.44375\n",
      "Unseen Data Accuracy: 0.461\n",
      "Early stopping after epoch 5\n",
      "Validation Accuracy: 0.44375\n",
      "Unseen Data Accuracy: 0.461\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# Scale the features to a specific range\n",
    "# Here, we are scaling the features to the range [0, 1]\n",
    "min_val = np.min(train_data, axis=0)\n",
    "max_val = np.max(train_data, axis=0)\n",
    "train_data = (train_data - min_val) / (max_val - min_val)\n",
    "val_data = (val_data - min_val) / (max_val - min_val)\n",
    "unseen_data = (unseen_data - min_val) / (max_val - min_val)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [100]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)\n",
    "\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with early stopping\n",
    "best_score = 0\n",
    "best_epoch = 0\n",
    "patience = 5\n",
    "for epoch in range(100):\n",
    "    best_classifier.fit(train_data, train_labels)\n",
    "    val_predictions = best_classifier.predict(val_data)\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    if val_accuracy > best_score:\n",
    "        best_score = val_accuracy\n",
    "        best_epoch = epoch\n",
    "    elif epoch - best_epoch >= patience:\n",
    "        print(\"Early stopping after epoch\", epoch)\n",
    "        break\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 20, 'n_estimators': 200}\n",
      "Validation Accuracy: 0.35484375\n",
      "Validation Accuracy: 0.3825\n",
      "Unseen Data Accuracy: 0.3945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# Generate polynomial features for the selected features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "train_data_poly = poly.fit_transform(train_data[:, [0, 2]])\n",
    "val_data_poly = poly.transform(val_data[:, [0, 2]])\n",
    "unseen_data_poly = poly.transform(unseen_data[:, [0, 2]])\n",
    "\n",
    "# Add the generated polynomial features to the dataset\n",
    "train_data = np.concatenate((train_data, train_data_poly), axis=1)\n",
    "val_data = np.concatenate((val_data, val_data_poly), axis=1)\n",
    "unseen_data = np.concatenate((unseen_data, unseen_data_poly), axis=1)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.3649472630023958\n",
      "Validation Loss: 2.247571759223938\n",
      "Epoch [2/50], Loss: 2.2175538611412047\n",
      "Validation Loss: 2.190909938812256\n",
      "Epoch [3/50], Loss: 2.1573812329769133\n",
      "Validation Loss: 2.1537196016311646\n",
      "Epoch [4/50], Loss: 2.1097389388084413\n",
      "Validation Loss: 2.116653664112091\n",
      "Epoch [5/50], Loss: 2.0499336910247803\n",
      "Validation Loss: 2.0933871030807496\n",
      "Epoch [6/50], Loss: 2.0078112477064134\n",
      "Validation Loss: 2.051489760875702\n",
      "Epoch [7/50], Loss: 1.977400752902031\n",
      "Validation Loss: 2.0149634504318237\n",
      "Epoch [8/50], Loss: 1.9455343747138978\n",
      "Validation Loss: 1.9862100958824158\n",
      "Epoch [9/50], Loss: 1.910859557390213\n",
      "Validation Loss: 1.954639995098114\n",
      "Epoch [10/50], Loss: 1.8744046711921691\n",
      "Validation Loss: 1.9266823244094848\n",
      "Epoch [11/50], Loss: 1.8166216152906418\n",
      "Validation Loss: 1.9193699669837951\n",
      "Epoch [12/50], Loss: 1.8208249682188034\n",
      "Validation Loss: 1.9141188549995423\n",
      "Epoch [13/50], Loss: 1.8179168301820754\n",
      "Validation Loss: 1.910422031879425\n",
      "Epoch [14/50], Loss: 1.806462236046791\n",
      "Validation Loss: 1.9109723496437072\n",
      "Epoch [15/50], Loss: 1.8022572350502015\n",
      "Validation Loss: 1.9058281326293944\n",
      "Epoch [16/50], Loss: 1.797824073433876\n",
      "Validation Loss: 1.909144515991211\n",
      "Epoch [17/50], Loss: 1.7873393630981445\n",
      "Validation Loss: 1.9060336661338806\n",
      "Epoch [18/50], Loss: 1.7876037353277205\n",
      "Validation Loss: 1.903714394569397\n",
      "Epoch [19/50], Loss: 1.7837731647491455\n",
      "Validation Loss: 1.8994276475906373\n",
      "Epoch [20/50], Loss: 1.7945612168312073\n",
      "Validation Loss: 1.8992122912406921\n",
      "Epoch [21/50], Loss: 1.7814382499456405\n",
      "Validation Loss: 1.8968765211105347\n",
      "Epoch [22/50], Loss: 1.7723158299922943\n",
      "Validation Loss: 1.8979786920547486\n",
      "Epoch [23/50], Loss: 1.7801908361911774\n",
      "Validation Loss: 1.898854591846466\n",
      "Epoch [24/50], Loss: 1.7811092728376388\n",
      "Validation Loss: 1.8969106769561768\n",
      "Epoch [25/50], Loss: 1.7667368483543395\n",
      "Validation Loss: 1.896367163658142\n",
      "Epoch [26/50], Loss: 1.7641467535495758\n",
      "Validation Loss: 1.896108522415161\n",
      "Epoch [27/50], Loss: 1.767355141043663\n",
      "Validation Loss: 1.8957729649543762\n",
      "Epoch [28/50], Loss: 1.7719362699985504\n",
      "Validation Loss: 1.8976766538619996\n",
      "Epoch [29/50], Loss: 1.7714021402597426\n",
      "Validation Loss: 1.8971127080917358\n",
      "Epoch [30/50], Loss: 1.7732111775875092\n",
      "Validation Loss: 1.8973770070075988\n",
      "Epoch [31/50], Loss: 1.774199297428131\n",
      "Validation Loss: 1.8963088154792787\n",
      "Epoch [32/50], Loss: 1.771597769856453\n",
      "Validation Loss: 1.8930452132225037\n",
      "Epoch [33/50], Loss: 1.77119140625\n",
      "Validation Loss: 1.89443532705307\n",
      "Epoch [34/50], Loss: 1.7686992609500884\n",
      "Validation Loss: 1.896952736377716\n",
      "Epoch [35/50], Loss: 1.7675168579816818\n",
      "Validation Loss: 1.8907124137878417\n",
      "Epoch [36/50], Loss: 1.7625437676906586\n",
      "Validation Loss: 1.8948881530761719\n",
      "Epoch [37/50], Loss: 1.7728210484981537\n",
      "Validation Loss: 1.8950264167785644\n",
      "Epoch [38/50], Loss: 1.7714079880714417\n",
      "Validation Loss: 1.8935307955741882\n",
      "Epoch [39/50], Loss: 1.7771085757017135\n",
      "Validation Loss: 1.8942856788635254\n",
      "Epoch [40/50], Loss: 1.7654463648796082\n",
      "Validation Loss: 1.895385239124298\n",
      "Early stopping.\n",
      "Validation Accuracy: 0.296875\n",
      "Accuracy of the network on the unseen data: 32 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv('traindata.txt', header=None, delimiter=',')\n",
    "X = df.values\n",
    "\n",
    "# Load the labels\n",
    "y = pd.read_csv('trainlabels.txt', header=None).values.ravel()\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "X_train, X_unseen, y_train, y_unseen = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model using PyTorch's nn.Module\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=X.shape[1], out_features=64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn2(torch.relu(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "X_unseen = torch.tensor(X_unseen, dtype=torch.float32)\n",
    "y_unseen = torch.tensor(y_unseen, dtype=torch.long)\n",
    "\n",
    "# Convert the tensors into datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "unseen_dataset = TensorDataset(X_unseen, y_unseen)\n",
    "\n",
    "# Convert the datasets into dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "unseen_loader = DataLoader(unseen_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create an instance of your model\n",
    "model = Classifier()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "no_improve = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f\"Validation Loss: {val_loss}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == 5:\n",
    "            print('Early stopping.')\n",
    "            break\n",
    "    scheduler.step()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict the labels for the validation data\n",
    "with torch.no_grad():\n",
    "    val_predictions = model(X_val)\n",
    "    _, val_predicted_labels = torch.max(val_predictions, 1)\n",
    "\n",
    "# Calculate and print the accuracy on the validation data\n",
    "val_accuracy = accuracy_score(y_val, val_predicted_labels)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Evaluation on unseen data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in unseen_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the unseen data: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0             1             2             3             4   \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       0.003283      0.045539     -0.001024     -0.002836      0.018141   \n",
      "std        0.545496      0.583899      0.543287      0.546328      0.567012   \n",
      "min       -2.230000     -2.240000     -2.210000     -1.830000     -2.270000   \n",
      "25%       -0.370000     -0.340000     -0.370000     -0.380000     -0.350000   \n",
      "50%       -0.000000      0.030000     -0.000000     -0.010000      0.010000   \n",
      "75%        0.360000      0.420000      0.360000      0.360000      0.390000   \n",
      "max        2.110000      2.370000      2.380000      2.050000      2.830000   \n",
      "\n",
      "                 5             6             7            8             9   \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.00000  10000.000000   \n",
      "mean       0.003151     -0.003583     -0.006363      0.14467      0.544114   \n",
      "std        0.565869      0.545105      0.551033      0.65011      0.743096   \n",
      "min       -2.020000     -1.970000     -2.190000     -2.30000     -2.120000   \n",
      "25%       -0.380000     -0.370000     -0.380000     -0.30000      0.010000   \n",
      "50%       -0.000000      0.000000      0.000000      0.10000      0.540000   \n",
      "75%        0.380000      0.360000      0.360000      0.54000      1.080000   \n",
      "max        2.520000      1.980000      2.130000      2.77000      3.040000   \n",
      "\n",
      "       ...            61            62            63            64  \\\n",
      "count  ...  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean   ...      0.121473      0.002367      0.003018      1.500600   \n",
      "std    ...      0.636421      0.548940      0.552322      1.113968   \n",
      "min    ...     -2.060000     -2.180000     -2.280000      0.000000   \n",
      "25%    ...     -0.320000     -0.370000     -0.380000      1.000000   \n",
      "50%    ...      0.090000     -0.000000     -0.000000      1.000000   \n",
      "75%    ...      0.510000      0.370000      0.370000      2.000000   \n",
      "max    ...      2.680000      2.780000      2.190000      3.000000   \n",
      "\n",
      "                 65            66            67            68            69  \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       4.539100      4.488300      4.464600      4.503400      0.006400   \n",
      "std        2.856482      2.880849      2.881593      2.874198      0.819894   \n",
      "min        0.000000      0.000000      0.000000      0.000000     -1.000000   \n",
      "25%        2.000000      2.000000      2.000000      2.000000     -1.000000   \n",
      "50%        5.000000      4.000000      4.000000      5.000000      0.000000   \n",
      "75%        7.000000      7.000000      7.000000      7.000000      1.000000   \n",
      "max        9.000000      9.000000      9.000000      9.000000      1.000000   \n",
      "\n",
      "                 70  \n",
      "count  10000.000000  \n",
      "mean       0.015400  \n",
      "std        0.815861  \n",
      "min       -1.000000  \n",
      "25%       -1.000000  \n",
      "50%        0.000000  \n",
      "75%        1.000000  \n",
      "max        1.000000  \n",
      "\n",
      "[8 rows x 71 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "print(train_data.describe())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MLPClassifier())\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'nn__activation': ['relu', 'tanh'],\n",
    "    'nn__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(train_data, train_labels.values.ravel())\n",
    "\n",
    "# Print the best hyperparameters and the accuracy\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Accuracy: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.9698333333333333\n",
      "Validation accuracy:  0.2215\n",
      "Test accuracy:  0.2475\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load the training data and labels\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(val_data, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "val_data_scaled = scaler.transform(val_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Train a neural network classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', learning_rate_init=0.01, alpha=0.001, random_state=42)\n",
    "clf.fit(train_data_scaled, train_labels.values.ravel())\n",
    "\n",
    "# Evaluate the performance on the training, validation, and unseen data\n",
    "train_acc = clf.score(train_data_scaled, train_labels)\n",
    "val_acc = clf.score(val_data_scaled, val_labels)\n",
    "test_acc = clf.score(test_data_scaled, test_labels)\n",
    "\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Validation accuracy: \", val_acc)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the training data and labels\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(val_data, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "val_data_scaled = scaler.transform(val_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MLPClassifier(max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (100, 50, 25)],\n",
    "    'nn__activation': ['relu', 'tanh'],\n",
    "    'nn__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(train_data, train_labels.values.ravel())\n",
    "\n",
    "# Print the best hyperparameters and the accuracy\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Accuracy: \", grid_search.best_score_)\n",
    "\n",
    "# Train a deep neural network classifier with the best hyperparameters\n",
    "clf = MLPClassifier(**grid_search.best_params_, max_iter=10000, random_state=42)\n",
    "clf.fit(train_data_scaled, train_labels.values.ravel())\n",
    "\n",
    "# Evaluate the performance on the training, validation, and unseen data\n",
    "train_acc = clf.score(train_data_scaled, train_labels)\n",
    "val_acc = clf.score(val_data_scaled, val_labels)\n",
    "test_acc = clf.score(test_data_scaled, test_labels)\n",
    "\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Validation accuracy: \", val_acc)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 64)                4608      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,834\n",
      "Trainable params: 21,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "657/657 [==============================] - 1s 769us/step - loss: 0.0876 - accuracy: 0.1835\n",
      "Epoch 2/100\n",
      "657/657 [==============================] - 1s 768us/step - loss: 0.0769 - accuracy: 0.3491\n",
      "Epoch 3/100\n",
      "657/657 [==============================] - 0s 758us/step - loss: 0.0653 - accuracy: 0.4840\n",
      "Epoch 4/100\n",
      "657/657 [==============================] - 1s 767us/step - loss: 0.0565 - accuracy: 0.5776\n",
      "Epoch 5/100\n",
      "657/657 [==============================] - 0s 759us/step - loss: 0.0494 - accuracy: 0.6451\n",
      "Epoch 6/100\n",
      "657/657 [==============================] - 1s 779us/step - loss: 0.0434 - accuracy: 0.6974\n",
      "Epoch 7/100\n",
      "657/657 [==============================] - 1s 762us/step - loss: 0.0381 - accuracy: 0.7417\n",
      "Epoch 8/100\n",
      "657/657 [==============================] - 1s 830us/step - loss: 0.0336 - accuracy: 0.7786\n",
      "Epoch 9/100\n",
      "657/657 [==============================] - 1s 787us/step - loss: 0.0302 - accuracy: 0.8026\n",
      "Epoch 10/100\n",
      "657/657 [==============================] - 1s 764us/step - loss: 0.0270 - accuracy: 0.8278\n",
      "Epoch 11/100\n",
      "657/657 [==============================] - 0s 760us/step - loss: 0.0246 - accuracy: 0.8443\n",
      "Epoch 12/100\n",
      "657/657 [==============================] - 0s 760us/step - loss: 0.0224 - accuracy: 0.8613\n",
      "Epoch 13/100\n",
      "657/657 [==============================] - 1s 766us/step - loss: 0.0210 - accuracy: 0.8691\n",
      "Epoch 14/100\n",
      "657/657 [==============================] - 1s 780us/step - loss: 0.0191 - accuracy: 0.8815\n",
      "Epoch 15/100\n",
      "657/657 [==============================] - 1s 789us/step - loss: 0.0180 - accuracy: 0.8889\n",
      "Epoch 16/100\n",
      "657/657 [==============================] - 1s 862us/step - loss: 0.0171 - accuracy: 0.8948\n",
      "Epoch 17/100\n",
      "657/657 [==============================] - 1s 847us/step - loss: 0.0164 - accuracy: 0.8982\n",
      "Epoch 18/100\n",
      "657/657 [==============================] - 1s 835us/step - loss: 0.0161 - accuracy: 0.9014\n",
      "Epoch 19/100\n",
      "657/657 [==============================] - 1s 799us/step - loss: 0.0148 - accuracy: 0.9077\n",
      "Epoch 20/100\n",
      "657/657 [==============================] - 1s 806us/step - loss: 0.0155 - accuracy: 0.9045\n",
      "Epoch 21/100\n",
      "657/657 [==============================] - 1s 840us/step - loss: 0.0149 - accuracy: 0.9077\n",
      "Epoch 22/100\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0137 - accuracy: 0.9159\n",
      "Epoch 23/100\n",
      "657/657 [==============================] - 1s 827us/step - loss: 0.0141 - accuracy: 0.9126\n",
      "Epoch 24/100\n",
      "657/657 [==============================] - 0s 753us/step - loss: 0.0142 - accuracy: 0.9120\n",
      "Epoch 25/100\n",
      "657/657 [==============================] - 0s 755us/step - loss: 0.0134 - accuracy: 0.9169\n",
      "Epoch 26/100\n",
      "657/657 [==============================] - 0s 752us/step - loss: 0.0135 - accuracy: 0.9169\n",
      "Epoch 27/100\n",
      "657/657 [==============================] - 1s 819us/step - loss: 0.0127 - accuracy: 0.9212\n",
      "Epoch 28/100\n",
      "657/657 [==============================] - 0s 748us/step - loss: 0.0123 - accuracy: 0.9238\n",
      "Epoch 29/100\n",
      "657/657 [==============================] - 0s 744us/step - loss: 0.0129 - accuracy: 0.9203\n",
      "Epoch 30/100\n",
      "657/657 [==============================] - 1s 824us/step - loss: 0.0129 - accuracy: 0.9211\n",
      "Epoch 31/100\n",
      "657/657 [==============================] - 1s 796us/step - loss: 0.0126 - accuracy: 0.9223\n",
      "Epoch 32/100\n",
      "657/657 [==============================] - 1s 928us/step - loss: 0.0122 - accuracy: 0.9250\n",
      "Epoch 33/100\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0118 - accuracy: 0.9271\n",
      "Epoch 34/100\n",
      "657/657 [==============================] - 1s 871us/step - loss: 0.0123 - accuracy: 0.9242\n",
      "Epoch 35/100\n",
      "657/657 [==============================] - 1s 905us/step - loss: 0.0122 - accuracy: 0.9247\n",
      "Epoch 36/100\n",
      "657/657 [==============================] - 1s 904us/step - loss: 0.0120 - accuracy: 0.9252\n",
      "Epoch 37/100\n",
      "657/657 [==============================] - 1s 854us/step - loss: 0.0117 - accuracy: 0.9273\n",
      "Epoch 38/100\n",
      "657/657 [==============================] - 1s 832us/step - loss: 0.0116 - accuracy: 0.9284\n",
      "Epoch 39/100\n",
      "657/657 [==============================] - 1s 872us/step - loss: 0.0112 - accuracy: 0.9314\n",
      "Epoch 40/100\n",
      "657/657 [==============================] - 1s 843us/step - loss: 0.0121 - accuracy: 0.9259\n",
      "Epoch 41/100\n",
      "657/657 [==============================] - 1s 874us/step - loss: 0.0117 - accuracy: 0.9274\n",
      "Epoch 42/100\n",
      "657/657 [==============================] - 1s 793us/step - loss: 0.0111 - accuracy: 0.9311\n",
      "Epoch 43/100\n",
      "657/657 [==============================] - 1s 797us/step - loss: 0.0111 - accuracy: 0.9319\n",
      "Epoch 44/100\n",
      "657/657 [==============================] - 1s 820us/step - loss: 0.0113 - accuracy: 0.9303\n",
      "Epoch 45/100\n",
      "158/657 [======>.......................] - ETA: 0s - loss: 0.0107 - accuracy: 0.9349"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "# X = pd.read_csv('traindata.txt',delimiter=',',header=None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "# Assuming X_train is a Pandas Series\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.001  # Adjust as necessary\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "# Define the NN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define an Adam optimizer with the desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs=100,\n",
    "                    batch_size=32)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_test_pred_prob = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Model accuracy:', accuracy)\n",
    "\n",
    "# Create confusion matrix\n",
    "# confusion_mat = confusion_matrix(y_test, y_test_pred)\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
