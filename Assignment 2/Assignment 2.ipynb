{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.334375\n",
      "Unseen Data Accuracy: 0.3795\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data\n",
    "val_predictions = classifier.predict(val_data)\n",
    "unseen_predictions = classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 50, 'n_estimators': 500}\n",
      "Validation Accuracy: 0.40265625\n",
      "Validation Accuracy: 0.42375\n",
      "Unseen Data Accuracy: 0.4485\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [5, 10, 20, None]\n",
    "# }\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 20, 'n_estimators': 200}\n",
      "Validation Accuracy: 0.36265625\n",
      "Validation Accuracy: 0.42875\n",
      "Unseen Data Accuracy: 0.443\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# Scale the features to a specific range\n",
    "# Here, we are scaling the features to the range [0, 1]\n",
    "min_val = np.min(train_data, axis=0)\n",
    "max_val = np.max(train_data, axis=0)\n",
    "train_data = (train_data - min_val) / (max_val - min_val)\n",
    "val_data = (val_data - min_val) / (max_val - min_val)\n",
    "unseen_data = (unseen_data - min_val) / (max_val - min_val)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 20, 'n_estimators': 200}\n",
      "Validation Accuracy: 0.35484375\n",
      "Validation Accuracy: 0.3825\n",
      "Unseen Data Accuracy: 0.3945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# Generate polynomial features for the selected features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "train_data_poly = poly.fit_transform(train_data[:, [0, 2]])\n",
    "val_data_poly = poly.transform(val_data[:, [0, 2]])\n",
    "unseen_data_poly = poly.transform(unseen_data[:, [0, 2]])\n",
    "\n",
    "# Add the generated polynomial features to the dataset\n",
    "train_data = np.concatenate((train_data, train_data_poly), axis=1)\n",
    "val_data = np.concatenate((val_data, val_data_poly), axis=1)\n",
    "unseen_data = np.concatenate((unseen_data, unseen_data_poly), axis=1)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.3649472630023958\n",
      "Validation Loss: 2.247571759223938\n",
      "Epoch [2/50], Loss: 2.2175538611412047\n",
      "Validation Loss: 2.190909938812256\n",
      "Epoch [3/50], Loss: 2.1573812329769133\n",
      "Validation Loss: 2.1537196016311646\n",
      "Epoch [4/50], Loss: 2.1097389388084413\n",
      "Validation Loss: 2.116653664112091\n",
      "Epoch [5/50], Loss: 2.0499336910247803\n",
      "Validation Loss: 2.0933871030807496\n",
      "Epoch [6/50], Loss: 2.0078112477064134\n",
      "Validation Loss: 2.051489760875702\n",
      "Epoch [7/50], Loss: 1.977400752902031\n",
      "Validation Loss: 2.0149634504318237\n",
      "Epoch [8/50], Loss: 1.9455343747138978\n",
      "Validation Loss: 1.9862100958824158\n",
      "Epoch [9/50], Loss: 1.910859557390213\n",
      "Validation Loss: 1.954639995098114\n",
      "Epoch [10/50], Loss: 1.8744046711921691\n",
      "Validation Loss: 1.9266823244094848\n",
      "Epoch [11/50], Loss: 1.8166216152906418\n",
      "Validation Loss: 1.9193699669837951\n",
      "Epoch [12/50], Loss: 1.8208249682188034\n",
      "Validation Loss: 1.9141188549995423\n",
      "Epoch [13/50], Loss: 1.8179168301820754\n",
      "Validation Loss: 1.910422031879425\n",
      "Epoch [14/50], Loss: 1.806462236046791\n",
      "Validation Loss: 1.9109723496437072\n",
      "Epoch [15/50], Loss: 1.8022572350502015\n",
      "Validation Loss: 1.9058281326293944\n",
      "Epoch [16/50], Loss: 1.797824073433876\n",
      "Validation Loss: 1.909144515991211\n",
      "Epoch [17/50], Loss: 1.7873393630981445\n",
      "Validation Loss: 1.9060336661338806\n",
      "Epoch [18/50], Loss: 1.7876037353277205\n",
      "Validation Loss: 1.903714394569397\n",
      "Epoch [19/50], Loss: 1.7837731647491455\n",
      "Validation Loss: 1.8994276475906373\n",
      "Epoch [20/50], Loss: 1.7945612168312073\n",
      "Validation Loss: 1.8992122912406921\n",
      "Epoch [21/50], Loss: 1.7814382499456405\n",
      "Validation Loss: 1.8968765211105347\n",
      "Epoch [22/50], Loss: 1.7723158299922943\n",
      "Validation Loss: 1.8979786920547486\n",
      "Epoch [23/50], Loss: 1.7801908361911774\n",
      "Validation Loss: 1.898854591846466\n",
      "Epoch [24/50], Loss: 1.7811092728376388\n",
      "Validation Loss: 1.8969106769561768\n",
      "Epoch [25/50], Loss: 1.7667368483543395\n",
      "Validation Loss: 1.896367163658142\n",
      "Epoch [26/50], Loss: 1.7641467535495758\n",
      "Validation Loss: 1.896108522415161\n",
      "Epoch [27/50], Loss: 1.767355141043663\n",
      "Validation Loss: 1.8957729649543762\n",
      "Epoch [28/50], Loss: 1.7719362699985504\n",
      "Validation Loss: 1.8976766538619996\n",
      "Epoch [29/50], Loss: 1.7714021402597426\n",
      "Validation Loss: 1.8971127080917358\n",
      "Epoch [30/50], Loss: 1.7732111775875092\n",
      "Validation Loss: 1.8973770070075988\n",
      "Epoch [31/50], Loss: 1.774199297428131\n",
      "Validation Loss: 1.8963088154792787\n",
      "Epoch [32/50], Loss: 1.771597769856453\n",
      "Validation Loss: 1.8930452132225037\n",
      "Epoch [33/50], Loss: 1.77119140625\n",
      "Validation Loss: 1.89443532705307\n",
      "Epoch [34/50], Loss: 1.7686992609500884\n",
      "Validation Loss: 1.896952736377716\n",
      "Epoch [35/50], Loss: 1.7675168579816818\n",
      "Validation Loss: 1.8907124137878417\n",
      "Epoch [36/50], Loss: 1.7625437676906586\n",
      "Validation Loss: 1.8948881530761719\n",
      "Epoch [37/50], Loss: 1.7728210484981537\n",
      "Validation Loss: 1.8950264167785644\n",
      "Epoch [38/50], Loss: 1.7714079880714417\n",
      "Validation Loss: 1.8935307955741882\n",
      "Epoch [39/50], Loss: 1.7771085757017135\n",
      "Validation Loss: 1.8942856788635254\n",
      "Epoch [40/50], Loss: 1.7654463648796082\n",
      "Validation Loss: 1.895385239124298\n",
      "Early stopping.\n",
      "Validation Accuracy: 0.296875\n",
      "Accuracy of the network on the unseen data: 32 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv('traindata.txt', header=None, delimiter=',')\n",
    "X = df.values\n",
    "\n",
    "# Load the labels\n",
    "y = pd.read_csv('trainlabels.txt', header=None).values.ravel()\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "X_train, X_unseen, y_train, y_unseen = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model using PyTorch's nn.Module\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=X.shape[1], out_features=64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn2(torch.relu(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "X_unseen = torch.tensor(X_unseen, dtype=torch.float32)\n",
    "y_unseen = torch.tensor(y_unseen, dtype=torch.long)\n",
    "\n",
    "# Convert the tensors into datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "unseen_dataset = TensorDataset(X_unseen, y_unseen)\n",
    "\n",
    "# Convert the datasets into dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "unseen_loader = DataLoader(unseen_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create an instance of your model\n",
    "model = Classifier()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "no_improve = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f\"Validation Loss: {val_loss}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == 5:\n",
    "            print('Early stopping.')\n",
    "            break\n",
    "    scheduler.step()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict the labels for the validation data\n",
    "with torch.no_grad():\n",
    "    val_predictions = model(X_val)\n",
    "    _, val_predicted_labels = torch.max(val_predictions, 1)\n",
    "\n",
    "# Calculate and print the accuracy on the validation data\n",
    "val_accuracy = accuracy_score(y_val, val_predicted_labels)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Evaluation on unseen data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in unseen_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the unseen data: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "print(train_data.describe())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MLPClassifier())\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'nn__activation': ['relu', 'tanh'],\n",
    "    'nn__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(train_data, train_labels.values.ravel())\n",
    "\n",
    "# Print the best hyperparameters and the accuracy\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Accuracy: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.9698333333333333\n",
      "Validation accuracy:  0.2215\n",
      "Test accuracy:  0.2475\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load the training data and labels\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(val_data, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "val_data_scaled = scaler.transform(val_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Train a neural network classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', learning_rate_init=0.01, alpha=0.001, random_state=42)\n",
    "clf.fit(train_data_scaled, train_labels.values.ravel())\n",
    "\n",
    "# Evaluate the performance on the training, validation, and unseen data\n",
    "train_acc = clf.score(train_data_scaled, train_labels)\n",
    "val_acc = clf.score(val_data_scaled, val_labels)\n",
    "test_acc = clf.score(test_data_scaled, test_labels)\n",
    "\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Validation accuracy: \", val_acc)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'nn__activation': 'relu', 'nn__alpha': 0.01, 'nn__hidden_layer_sizes': (100, 100), 'nn__learning_rate_init': 0.01}\n",
      "Accuracy:  0.30583333333333335\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLPClassifier.__init__() got an unexpected keyword argument 'nn__activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m\"\u001b[39m, grid_search\u001b[39m.\u001b[39mbest_score_)\n\u001b[0;32m     44\u001b[0m \u001b[39m# Train a deep neural network classifier with the best hyperparameters\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m clf \u001b[39m=\u001b[39m MLPClassifier(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgrid_search\u001b[39m.\u001b[39;49mbest_params_, max_iter\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[0;32m     46\u001b[0m clf\u001b[39m.\u001b[39mfit(train_data_scaled, train_labels\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel())\n\u001b[0;32m     48\u001b[0m \u001b[39m# Evaluate the performance on the training, validation, and unseen data\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: MLPClassifier.__init__() got an unexpected keyword argument 'nn__activation'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the training data and labels\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(val_data, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "val_data_scaled = scaler.transform(val_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MLPClassifier(max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (100, 50, 25)],\n",
    "    'nn__activation': ['relu', 'tanh'],\n",
    "    'nn__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(train_data, train_labels.values.ravel())\n",
    "\n",
    "# Print the best hyperparameters and the accuracy\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Accuracy: \", grid_search.best_score_)\n",
    "\n",
    "# Train a deep neural network classifier with the best hyperparameters\n",
    "clf = MLPClassifier(**grid_search.best_params_, max_iter=10000, random_state=42)\n",
    "clf.fit(train_data_scaled, train_labels.values.ravel())\n",
    "\n",
    "# Evaluate the performance on the training, validation, and unseen data\n",
    "train_acc = clf.score(train_data_scaled, train_labels)\n",
    "val_acc = clf.score(val_data_scaled, val_labels)\n",
    "test_acc = clf.score(test_data_scaled, test_labels)\n",
    "\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Validation accuracy: \", val_acc)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
