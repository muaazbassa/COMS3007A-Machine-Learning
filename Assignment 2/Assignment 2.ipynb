{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.334375\n",
      "Unseen Data Accuracy: 0.3795\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data\n",
    "val_predictions = classifier.predict(val_data)\n",
    "unseen_predictions = classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 50, 'n_estimators': 500}\n",
      "Validation Accuracy: 0.40265625\n",
      "Validation Accuracy: 0.42375\n",
      "Unseen Data Accuracy: 0.4485\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [5, 10, 20, None]\n",
    "# }\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 100, 'n_estimators': 1000}\n",
      "Validation Accuracy: 0.41734375\n",
      "Validation Accuracy: 0.44375\n",
      "Unseen Data Accuracy: 0.461\n",
      "Early stopping after epoch 5\n",
      "Validation Accuracy: 0.44375\n",
      "Unseen Data Accuracy: 0.461\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# Scale the features to a specific range\n",
    "# Here, we are scaling the features to the range [0, 1]\n",
    "min_val = np.min(train_data, axis=0)\n",
    "max_val = np.max(train_data, axis=0)\n",
    "train_data = (train_data - min_val) / (max_val - min_val)\n",
    "val_data = (val_data - min_val) / (max_val - min_val)\n",
    "unseen_data = (unseen_data - min_val) / (max_val - min_val)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [100]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)\n",
    "\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with early stopping\n",
    "best_score = 0\n",
    "best_epoch = 0\n",
    "patience = 5\n",
    "for epoch in range(100):\n",
    "    best_classifier.fit(train_data, train_labels)\n",
    "    val_predictions = best_classifier.predict(val_data)\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    if val_accuracy > best_score:\n",
    "        best_score = val_accuracy\n",
    "        best_epoch = epoch\n",
    "    elif epoch - best_epoch >= patience:\n",
    "        print(\"Early stopping after epoch\", epoch)\n",
    "        break\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 20, 'n_estimators': 200}\n",
      "Validation Accuracy: 0.35484375\n",
      "Validation Accuracy: 0.3825\n",
      "Unseen Data Accuracy: 0.3945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt('traindata.txt', delimiter=',')\n",
    "train_labels = np.loadtxt('trainlabels.txt')\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, unseen_data, train_labels, unseen_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "unseen_data = scaler.transform(unseen_data)\n",
    "\n",
    "# Remove correlated features\n",
    "corr_matrix = pd.DataFrame(train_data).corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "train_data = np.delete(train_data, to_drop, axis=1)\n",
    "val_data = np.delete(val_data, to_drop, axis=1)\n",
    "unseen_data = np.delete(unseen_data, to_drop, axis=1)\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "train_data = selector.fit_transform(train_data)\n",
    "val_data = selector.transform(val_data)\n",
    "unseen_data = selector.transform(unseen_data)\n",
    "\n",
    "# Generate polynomial features for the selected features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "train_data_poly = poly.fit_transform(train_data[:, [0, 2]])\n",
    "val_data_poly = poly.transform(val_data[:, [0, 2]])\n",
    "unseen_data_poly = poly.transform(unseen_data[:, [0, 2]])\n",
    "\n",
    "# Add the generated polynomial features to the dataset\n",
    "train_data = np.concatenate((train_data, train_data_poly), axis=1)\n",
    "val_data = np.concatenate((val_data, val_data_poly), axis=1)\n",
    "unseen_data = np.concatenate((unseen_data, unseen_data_poly), axis=1)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search object\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and their validation accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_classifier = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], \n",
    "                                          max_depth=grid_search.best_params_['max_depth'], \n",
    "                                          random_state=42)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_classifier.fit(train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the validation and unseen data using the best classifier\n",
    "val_predictions = best_classifier.predict(val_data)\n",
    "unseen_predictions = best_classifier.predict(unseen_data)\n",
    "\n",
    "# Calculate and print the accuracy on the validation and unseen data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Unseen Data Accuracy:\", unseen_accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.3649472630023958\n",
      "Validation Loss: 2.247571759223938\n",
      "Epoch [2/50], Loss: 2.2175538611412047\n",
      "Validation Loss: 2.190909938812256\n",
      "Epoch [3/50], Loss: 2.1573812329769133\n",
      "Validation Loss: 2.1537196016311646\n",
      "Epoch [4/50], Loss: 2.1097389388084413\n",
      "Validation Loss: 2.116653664112091\n",
      "Epoch [5/50], Loss: 2.0499336910247803\n",
      "Validation Loss: 2.0933871030807496\n",
      "Epoch [6/50], Loss: 2.0078112477064134\n",
      "Validation Loss: 2.051489760875702\n",
      "Epoch [7/50], Loss: 1.977400752902031\n",
      "Validation Loss: 2.0149634504318237\n",
      "Epoch [8/50], Loss: 1.9455343747138978\n",
      "Validation Loss: 1.9862100958824158\n",
      "Epoch [9/50], Loss: 1.910859557390213\n",
      "Validation Loss: 1.954639995098114\n",
      "Epoch [10/50], Loss: 1.8744046711921691\n",
      "Validation Loss: 1.9266823244094848\n",
      "Epoch [11/50], Loss: 1.8166216152906418\n",
      "Validation Loss: 1.9193699669837951\n",
      "Epoch [12/50], Loss: 1.8208249682188034\n",
      "Validation Loss: 1.9141188549995423\n",
      "Epoch [13/50], Loss: 1.8179168301820754\n",
      "Validation Loss: 1.910422031879425\n",
      "Epoch [14/50], Loss: 1.806462236046791\n",
      "Validation Loss: 1.9109723496437072\n",
      "Epoch [15/50], Loss: 1.8022572350502015\n",
      "Validation Loss: 1.9058281326293944\n",
      "Epoch [16/50], Loss: 1.797824073433876\n",
      "Validation Loss: 1.909144515991211\n",
      "Epoch [17/50], Loss: 1.7873393630981445\n",
      "Validation Loss: 1.9060336661338806\n",
      "Epoch [18/50], Loss: 1.7876037353277205\n",
      "Validation Loss: 1.903714394569397\n",
      "Epoch [19/50], Loss: 1.7837731647491455\n",
      "Validation Loss: 1.8994276475906373\n",
      "Epoch [20/50], Loss: 1.7945612168312073\n",
      "Validation Loss: 1.8992122912406921\n",
      "Epoch [21/50], Loss: 1.7814382499456405\n",
      "Validation Loss: 1.8968765211105347\n",
      "Epoch [22/50], Loss: 1.7723158299922943\n",
      "Validation Loss: 1.8979786920547486\n",
      "Epoch [23/50], Loss: 1.7801908361911774\n",
      "Validation Loss: 1.898854591846466\n",
      "Epoch [24/50], Loss: 1.7811092728376388\n",
      "Validation Loss: 1.8969106769561768\n",
      "Epoch [25/50], Loss: 1.7667368483543395\n",
      "Validation Loss: 1.896367163658142\n",
      "Epoch [26/50], Loss: 1.7641467535495758\n",
      "Validation Loss: 1.896108522415161\n",
      "Epoch [27/50], Loss: 1.767355141043663\n",
      "Validation Loss: 1.8957729649543762\n",
      "Epoch [28/50], Loss: 1.7719362699985504\n",
      "Validation Loss: 1.8976766538619996\n",
      "Epoch [29/50], Loss: 1.7714021402597426\n",
      "Validation Loss: 1.8971127080917358\n",
      "Epoch [30/50], Loss: 1.7732111775875092\n",
      "Validation Loss: 1.8973770070075988\n",
      "Epoch [31/50], Loss: 1.774199297428131\n",
      "Validation Loss: 1.8963088154792787\n",
      "Epoch [32/50], Loss: 1.771597769856453\n",
      "Validation Loss: 1.8930452132225037\n",
      "Epoch [33/50], Loss: 1.77119140625\n",
      "Validation Loss: 1.89443532705307\n",
      "Epoch [34/50], Loss: 1.7686992609500884\n",
      "Validation Loss: 1.896952736377716\n",
      "Epoch [35/50], Loss: 1.7675168579816818\n",
      "Validation Loss: 1.8907124137878417\n",
      "Epoch [36/50], Loss: 1.7625437676906586\n",
      "Validation Loss: 1.8948881530761719\n",
      "Epoch [37/50], Loss: 1.7728210484981537\n",
      "Validation Loss: 1.8950264167785644\n",
      "Epoch [38/50], Loss: 1.7714079880714417\n",
      "Validation Loss: 1.8935307955741882\n",
      "Epoch [39/50], Loss: 1.7771085757017135\n",
      "Validation Loss: 1.8942856788635254\n",
      "Epoch [40/50], Loss: 1.7654463648796082\n",
      "Validation Loss: 1.895385239124298\n",
      "Early stopping.\n",
      "Validation Accuracy: 0.296875\n",
      "Accuracy of the network on the unseen data: 32 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv('traindata.txt', header=None, delimiter=',')\n",
    "X = df.values\n",
    "\n",
    "# Load the labels\n",
    "y = pd.read_csv('trainlabels.txt', header=None).values.ravel()\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "X_train, X_unseen, y_train, y_unseen = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model using PyTorch's nn.Module\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=X.shape[1], out_features=64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn2(torch.relu(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "X_unseen = torch.tensor(X_unseen, dtype=torch.float32)\n",
    "y_unseen = torch.tensor(y_unseen, dtype=torch.long)\n",
    "\n",
    "# Convert the tensors into datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "unseen_dataset = TensorDataset(X_unseen, y_unseen)\n",
    "\n",
    "# Convert the datasets into dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "unseen_loader = DataLoader(unseen_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create an instance of your model\n",
    "model = Classifier()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "no_improve = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f\"Validation Loss: {val_loss}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == 5:\n",
    "            print('Early stopping.')\n",
    "            break\n",
    "    scheduler.step()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict the labels for the validation data\n",
    "with torch.no_grad():\n",
    "    val_predictions = model(X_val)\n",
    "    _, val_predicted_labels = torch.max(val_predictions, 1)\n",
    "\n",
    "# Calculate and print the accuracy on the validation data\n",
    "val_accuracy = accuracy_score(y_val, val_predicted_labels)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Evaluation on unseen data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in unseen_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the unseen data: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0             1             2             3             4   \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       0.003283      0.045539     -0.001024     -0.002836      0.018141   \n",
      "std        0.545496      0.583899      0.543287      0.546328      0.567012   \n",
      "min       -2.230000     -2.240000     -2.210000     -1.830000     -2.270000   \n",
      "25%       -0.370000     -0.340000     -0.370000     -0.380000     -0.350000   \n",
      "50%       -0.000000      0.030000     -0.000000     -0.010000      0.010000   \n",
      "75%        0.360000      0.420000      0.360000      0.360000      0.390000   \n",
      "max        2.110000      2.370000      2.380000      2.050000      2.830000   \n",
      "\n",
      "                 5             6             7            8             9   \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.00000  10000.000000   \n",
      "mean       0.003151     -0.003583     -0.006363      0.14467      0.544114   \n",
      "std        0.565869      0.545105      0.551033      0.65011      0.743096   \n",
      "min       -2.020000     -1.970000     -2.190000     -2.30000     -2.120000   \n",
      "25%       -0.380000     -0.370000     -0.380000     -0.30000      0.010000   \n",
      "50%       -0.000000      0.000000      0.000000      0.10000      0.540000   \n",
      "75%        0.380000      0.360000      0.360000      0.54000      1.080000   \n",
      "max        2.520000      1.980000      2.130000      2.77000      3.040000   \n",
      "\n",
      "       ...            61            62            63            64  \\\n",
      "count  ...  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean   ...      0.121473      0.002367      0.003018      1.500600   \n",
      "std    ...      0.636421      0.548940      0.552322      1.113968   \n",
      "min    ...     -2.060000     -2.180000     -2.280000      0.000000   \n",
      "25%    ...     -0.320000     -0.370000     -0.380000      1.000000   \n",
      "50%    ...      0.090000     -0.000000     -0.000000      1.000000   \n",
      "75%    ...      0.510000      0.370000      0.370000      2.000000   \n",
      "max    ...      2.680000      2.780000      2.190000      3.000000   \n",
      "\n",
      "                 65            66            67            68            69  \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       4.539100      4.488300      4.464600      4.503400      0.006400   \n",
      "std        2.856482      2.880849      2.881593      2.874198      0.819894   \n",
      "min        0.000000      0.000000      0.000000      0.000000     -1.000000   \n",
      "25%        2.000000      2.000000      2.000000      2.000000     -1.000000   \n",
      "50%        5.000000      4.000000      4.000000      5.000000      0.000000   \n",
      "75%        7.000000      7.000000      7.000000      7.000000      1.000000   \n",
      "max        9.000000      9.000000      9.000000      9.000000      1.000000   \n",
      "\n",
      "                 70  \n",
      "count  10000.000000  \n",
      "mean       0.015400  \n",
      "std        0.815861  \n",
      "min       -1.000000  \n",
      "25%       -1.000000  \n",
      "50%        0.000000  \n",
      "75%        1.000000  \n",
      "max        1.000000  \n",
      "\n",
      "[8 rows x 71 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "print(train_data.describe())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MLPClassifier())\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'nn__activation': ['relu', 'tanh'],\n",
    "    'nn__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(train_data, train_labels.values.ravel())\n",
    "\n",
    "# Print the best hyperparameters and the accuracy\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Accuracy: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.9698333333333333\n",
      "Validation accuracy:  0.2215\n",
      "Test accuracy:  0.2475\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load the training data and labels\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(val_data, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "val_data_scaled = scaler.transform(val_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Train a neural network classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', learning_rate_init=0.01, alpha=0.001, random_state=42)\n",
    "clf.fit(train_data_scaled, train_labels.values.ravel())\n",
    "\n",
    "# Evaluate the performance on the training, validation, and unseen data\n",
    "train_acc = clf.score(train_data_scaled, train_labels)\n",
    "val_acc = clf.score(val_data_scaled, val_labels)\n",
    "test_acc = clf.score(test_data_scaled, test_labels)\n",
    "\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Validation accuracy: \", val_acc)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the training data and labels\n",
    "train_data = pd.read_csv('traindata.txt', header=None)\n",
    "train_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split the data into training, validation, and unseen data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(val_data, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "val_data_scaled = scaler.transform(val_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MLPClassifier(max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (100, 50, 25)],\n",
    "    'nn__activation': ['relu', 'tanh'],\n",
    "    'nn__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(train_data, train_labels.values.ravel())\n",
    "\n",
    "# Print the best hyperparameters and the accuracy\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Accuracy: \", grid_search.best_score_)\n",
    "\n",
    "# Train a deep neural network classifier with the best hyperparameters\n",
    "clf = MLPClassifier(**grid_search.best_params_, max_iter=10000, random_state=42)\n",
    "clf.fit(train_data_scaled, train_labels.values.ravel())\n",
    "\n",
    "# Evaluate the performance on the training, validation, and unseen data\n",
    "train_acc = clf.score(train_data_scaled, train_labels)\n",
    "val_acc = clf.score(val_data_scaled, val_labels)\n",
    "test_acc = clf.score(test_data_scaled, test_labels)\n",
    "\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Validation accuracy: \", val_acc)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_177 (Dense)           (None, 64)                4608      \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " flatten_43 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_180 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_181 (Dense)           (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,594\n",
      "Trainable params: 23,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "657/657 [==============================] - 1s 885us/step - loss: 0.0886 - accuracy: 0.1491\n",
      "Epoch 2/30\n",
      "657/657 [==============================] - 1s 884us/step - loss: 0.0860 - accuracy: 0.2070\n",
      "Epoch 3/30\n",
      "657/657 [==============================] - 1s 886us/step - loss: 0.0828 - accuracy: 0.2583\n",
      "Epoch 4/30\n",
      "657/657 [==============================] - 1s 934us/step - loss: 0.0797 - accuracy: 0.2987\n",
      "Epoch 5/30\n",
      "657/657 [==============================] - 1s 891us/step - loss: 0.0776 - accuracy: 0.3288\n",
      "Epoch 6/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0755 - accuracy: 0.3550\n",
      "Epoch 7/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0736 - accuracy: 0.3736\n",
      "Epoch 8/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0718 - accuracy: 0.3970\n",
      "Epoch 9/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0706 - accuracy: 0.4077\n",
      "Epoch 10/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0692 - accuracy: 0.4212\n",
      "Epoch 11/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0687 - accuracy: 0.4244\n",
      "Epoch 12/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0679 - accuracy: 0.4325\n",
      "Epoch 13/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0663 - accuracy: 0.4471\n",
      "Epoch 14/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0658 - accuracy: 0.4529\n",
      "Epoch 15/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0655 - accuracy: 0.4541\n",
      "Epoch 16/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0643 - accuracy: 0.4660\n",
      "Epoch 17/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0637 - accuracy: 0.4710\n",
      "Epoch 18/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0630 - accuracy: 0.4773\n",
      "Epoch 19/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0627 - accuracy: 0.4830\n",
      "Epoch 20/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0625 - accuracy: 0.4848\n",
      "Epoch 21/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0636 - accuracy: 0.4750\n",
      "Epoch 22/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0628 - accuracy: 0.4826\n",
      "Epoch 23/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0625 - accuracy: 0.4831\n",
      "Epoch 24/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0607 - accuracy: 0.4993\n",
      "Epoch 25/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0613 - accuracy: 0.4915\n",
      "Epoch 26/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0612 - accuracy: 0.4938\n",
      "Epoch 27/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0626 - accuracy: 0.4843\n",
      "Epoch 28/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0613 - accuracy: 0.4905\n",
      "Epoch 29/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0602 - accuracy: 0.5011\n",
      "Epoch 30/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0608 - accuracy: 0.5001\n",
      "94/94 [==============================] - 0s 575us/step\n",
      "Model accuracy: 0.3436666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "# X = pd.read_csv('traindata.txt',delimiter=',',header=None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "# Assuming X_train is a Pandas Series\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71,)  # Number of input features\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.01  # Adjust as necessary\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "# Define the NN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define an Adam optimizer with the desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs=30,\n",
    "                    batch_size=32)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_test_pred_prob = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Model accuracy:', accuracy)\n",
    "\n",
    "# Create confusion matrix\n",
    "# confusion_mat = confusion_matrix(y_test, y_test_pred)\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_119 (Dense)           (None, 128)               9216      \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " normalization_17 (Normaliza  (None, 64)               129       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " re_lu_17 (ReLU)             (None, 64)                0         \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,443\n",
      "Trainable params: 102,314\n",
      "Non-trainable params: 129\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "657/657 [==============================] - 2s 1ms/step - loss: 2.2182 - accuracy: 0.1561\n",
      "Epoch 2/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 1.8556 - accuracy: 0.3212\n",
      "Epoch 3/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 1.4593 - accuracy: 0.4907\n",
      "Epoch 4/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 1.1216 - accuracy: 0.6164\n",
      "Epoch 5/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.8400 - accuracy: 0.7134\n",
      "Epoch 6/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.6437 - accuracy: 0.7822\n",
      "Epoch 7/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.4854 - accuracy: 0.8355\n",
      "Epoch 8/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.3806 - accuracy: 0.8730\n",
      "Epoch 9/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.2915 - accuracy: 0.9023\n",
      "Epoch 10/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.2567 - accuracy: 0.9139\n",
      "Epoch 11/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.2210 - accuracy: 0.9257\n",
      "Epoch 12/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.2071 - accuracy: 0.9311\n",
      "Epoch 13/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.9401\n",
      "Epoch 14/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.1600 - accuracy: 0.9481\n",
      "Epoch 15/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.1682 - accuracy: 0.9447\n",
      "Epoch 16/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.1458 - accuracy: 0.9507\n",
      "Epoch 17/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.1255 - accuracy: 0.9585\n",
      "Epoch 18/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.1486 - accuracy: 0.9521\n",
      "Epoch 19/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.1127 - accuracy: 0.9638\n",
      "Epoch 20/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.1230 - accuracy: 0.9593\n",
      "Epoch 21/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.1203 - accuracy: 0.9609\n",
      "Epoch 22/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.1111 - accuracy: 0.9635\n",
      "Epoch 23/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0955 - accuracy: 0.9688\n",
      "Epoch 24/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.1179 - accuracy: 0.9624\n",
      "Epoch 25/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0800 - accuracy: 0.9750\n",
      "Epoch 26/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.1156 - accuracy: 0.9630\n",
      "Epoch 27/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0975 - accuracy: 0.9687\n",
      "Epoch 28/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0927 - accuracy: 0.9703\n",
      "Epoch 29/30\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0973 - accuracy: 0.9690\n",
      "Epoch 30/30\n",
      "657/657 [==============================] - 1s 1ms/step - loss: 0.0905 - accuracy: 0.9714\n",
      "94/94 [==============================] - 0s 788us/step\n",
      "Model accuracy: 0.43166666666666664\n",
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_126 (Dense)           (None, 128)               9216      \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " flatten_18 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " normalization_18 (Normaliza  (None, 64)               129       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " re_lu_18 (ReLU)             (None, 64)                0         \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,443\n",
      "Trainable params: 102,314\n",
      "Non-trainable params: 129\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "657/657 [==============================] - 3s 3ms/step - loss: 2.1988 - accuracy: 0.1681\n",
      "Epoch 2/30\n",
      "333/657 [==============>...............] - ETA: 0s - loss: 1.8896 - accuracy: 0.3068"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     97\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train_encoded,\n\u001b[0;32m     99\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[0;32m    100\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[0;32m    102\u001b[0m \u001b[39m# Predicting the test set results\u001b[39;00m\n\u001b[0;32m    103\u001b[0m y_test_pred_prob \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1676\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1674\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1675\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1676\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   1677\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m         ):\n\u001b[0;32m   1684\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1376\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1377\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1378\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1380\u001b[0m )\n\u001b[0;32m   1382\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    646\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    648\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:774\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \n\u001b[0;32m    767\u001b[0m \u001b[39mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mRead\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 774\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n\u001b[0;32m    775\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:753\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    751\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 753\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    756\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    757\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m   tape\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    759\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    760\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    761\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:743\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    742\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 743\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    745\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    746\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:580\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    579\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 580\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    581\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    582\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    583\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "for i in range(10):\n",
    "    df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "\n",
    "    df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "    # Split df_features into X_train and X_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_features,\n",
    "        df_labels,\n",
    "        test_size=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(df_features.shape)\n",
    "    print(df_labels.shape)\n",
    "    # Assuming X_train is a Pandas Series\n",
    "    # Data augmentation - random perturbations\n",
    "    augmented_X_train = []\n",
    "    augmented_y_train = []\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        original_data = X_train.iloc[i].to_numpy()\n",
    "        augmented_X_train.append(original_data)\n",
    "        augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "        # Apply random perturbations\n",
    "        perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "        augmented_X_train.append(perturbed_data)\n",
    "        augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Convert augmented data to DataFrames\n",
    "    augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "    augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "    # Concatenate augmented data with original data\n",
    "    X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "    y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "    # Shuffle the augmented data\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "    # Define some constants\n",
    "    INPUT_SHAPE = (71,)  # Number of input features\n",
    "    NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "    LEARNING_RATE = 0.001  # Adjust as necessary\n",
    "\n",
    "    # One-hot encoding of output\n",
    "    y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "\n",
    "    # Define the NN architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Normalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # # Compile the model with the custom optimizer\n",
    "    # model.compile(optimizer=optimizer,\n",
    "    #               loss='mean_squared_error',\n",
    "    #               metrics=['accuracy'])\n",
    "\n",
    "      # Compile the model with the custom optimizer\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train_encoded,\n",
    "                        epochs=30,\n",
    "                        batch_size=32)\n",
    "\n",
    "    # Predicting the test set results\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model accuracy:', accuracy)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    # confusion_mat = confusion_matrix(y_test, y_test_pred)\n",
    "    # print('Confusion Matrix:')    # print(confusion_mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_7 (Conv1D)           (None, 69, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 34, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 1088)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               139392    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,810\n",
      "Trainable params: 140,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0894 - accuracy: 0.1448\n",
      "Epoch 2/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0878 - accuracy: 0.1956\n",
      "Epoch 3/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0865 - accuracy: 0.2249\n",
      "Epoch 4/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0854 - accuracy: 0.2503\n",
      "Epoch 5/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.2710\n",
      "Epoch 6/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.2892\n",
      "Epoch 7/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0821 - accuracy: 0.3052\n",
      "Epoch 8/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0810 - accuracy: 0.3241\n",
      "Epoch 9/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0797 - accuracy: 0.3439\n",
      "Epoch 10/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0784 - accuracy: 0.3602\n",
      "Epoch 11/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0771 - accuracy: 0.3781\n",
      "Epoch 12/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.3931\n",
      "Epoch 13/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.4087\n",
      "Epoch 14/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.4219\n",
      "Epoch 15/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0718 - accuracy: 0.4392\n",
      "Epoch 16/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0704 - accuracy: 0.4569\n",
      "Epoch 17/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0693 - accuracy: 0.4654\n",
      "Epoch 18/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0681 - accuracy: 0.4788\n",
      "Epoch 19/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0668 - accuracy: 0.4905\n",
      "Epoch 20/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0657 - accuracy: 0.5068\n",
      "Epoch 21/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0645 - accuracy: 0.5142\n",
      "Epoch 22/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0633 - accuracy: 0.5268\n",
      "Epoch 23/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0625 - accuracy: 0.5343\n",
      "Epoch 24/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0618 - accuracy: 0.5429\n",
      "Epoch 25/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0603 - accuracy: 0.5555\n",
      "Epoch 26/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0594 - accuracy: 0.5665\n",
      "Epoch 27/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0585 - accuracy: 0.5735\n",
      "Epoch 28/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0574 - accuracy: 0.5817\n",
      "Epoch 29/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0566 - accuracy: 0.5890\n",
      "Epoch 30/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0557 - accuracy: 0.5998\n",
      "Epoch 31/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.6032\n",
      "Epoch 32/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.6148\n",
      "Epoch 33/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0530 - accuracy: 0.6216\n",
      "Epoch 34/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.6243\n",
      "Epoch 35/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.6345\n",
      "Epoch 36/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0510 - accuracy: 0.6410\n",
      "Epoch 37/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.6440\n",
      "Epoch 38/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.6551\n",
      "Epoch 39/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0487 - accuracy: 0.6608\n",
      "Epoch 40/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0480 - accuracy: 0.6650\n",
      "Epoch 41/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0475 - accuracy: 0.6691\n",
      "Epoch 42/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0469 - accuracy: 0.6753\n",
      "Epoch 43/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0462 - accuracy: 0.6792\n",
      "Epoch 44/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0454 - accuracy: 0.6875\n",
      "Epoch 45/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0451 - accuracy: 0.6899\n",
      "Epoch 46/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0442 - accuracy: 0.6941\n",
      "Epoch 47/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0440 - accuracy: 0.6990\n",
      "Epoch 48/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0434 - accuracy: 0.7008\n",
      "Epoch 49/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0426 - accuracy: 0.7088\n",
      "Epoch 50/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.7135\n",
      "Epoch 51/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0418 - accuracy: 0.7145\n",
      "Epoch 52/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.7185\n",
      "Epoch 53/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0406 - accuracy: 0.7234\n",
      "Epoch 54/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0402 - accuracy: 0.7253\n",
      "Epoch 55/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0398 - accuracy: 0.7285\n",
      "Epoch 56/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.7355\n",
      "Epoch 57/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.7396\n",
      "Epoch 58/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.7410\n",
      "Epoch 59/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0378 - accuracy: 0.7467\n",
      "Epoch 60/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.7517\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "Model accuracy: 0.3\n",
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_8 (Conv1D)           (None, 69, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 34, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 1088)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               139392    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,810\n",
      "Trainable params: 140,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0897 - accuracy: 0.1302\n",
      "Epoch 2/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0879 - accuracy: 0.1849\n",
      "Epoch 3/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.2187\n",
      "Epoch 4/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0856 - accuracy: 0.2395\n",
      "Epoch 5/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0846 - accuracy: 0.2657\n",
      "Epoch 6/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0835 - accuracy: 0.2835\n",
      "Epoch 7/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0824 - accuracy: 0.3041\n",
      "Epoch 8/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0814 - accuracy: 0.3224\n",
      "Epoch 9/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0801 - accuracy: 0.3400\n",
      "Epoch 10/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.3570\n",
      "Epoch 11/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0774 - accuracy: 0.3775\n",
      "Epoch 12/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0761 - accuracy: 0.3965\n",
      "Epoch 13/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0746 - accuracy: 0.4100\n",
      "Epoch 14/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0734 - accuracy: 0.4241\n",
      "Epoch 15/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0719 - accuracy: 0.4445\n",
      "Epoch 16/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0704 - accuracy: 0.4570\n",
      "Epoch 17/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0692 - accuracy: 0.4715\n",
      "Epoch 18/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.4822\n",
      "Epoch 19/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0670 - accuracy: 0.4912\n",
      "Epoch 20/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0656 - accuracy: 0.5080\n",
      "Epoch 21/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0644 - accuracy: 0.5136\n",
      "Epoch 22/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0634 - accuracy: 0.5267\n",
      "Epoch 23/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0620 - accuracy: 0.5405\n",
      "Epoch 24/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0610 - accuracy: 0.5481\n",
      "Epoch 25/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0600 - accuracy: 0.5574\n",
      "Epoch 26/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0590 - accuracy: 0.5655\n",
      "Epoch 27/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0581 - accuracy: 0.5745\n",
      "Epoch 28/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0575 - accuracy: 0.5818\n",
      "Epoch 29/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0565 - accuracy: 0.5893\n",
      "Epoch 30/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0555 - accuracy: 0.6010\n",
      "Epoch 31/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0549 - accuracy: 0.6006\n",
      "Epoch 32/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.6136\n",
      "Epoch 33/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0533 - accuracy: 0.6174\n",
      "Epoch 34/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.6277\n",
      "Epoch 35/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0514 - accuracy: 0.6354\n",
      "Epoch 36/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0506 - accuracy: 0.6410\n",
      "Epoch 37/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0500 - accuracy: 0.6446\n",
      "Epoch 38/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0493 - accuracy: 0.6554\n",
      "Epoch 39/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0486 - accuracy: 0.6566\n",
      "Epoch 40/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0481 - accuracy: 0.6621\n",
      "Epoch 41/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0476 - accuracy: 0.6667\n",
      "Epoch 42/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0465 - accuracy: 0.6754\n",
      "Epoch 43/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.6809\n",
      "Epoch 44/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0455 - accuracy: 0.6838\n",
      "Epoch 45/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0448 - accuracy: 0.6892\n",
      "Epoch 46/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0441 - accuracy: 0.6934\n",
      "Epoch 47/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0435 - accuracy: 0.7000\n",
      "Epoch 48/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0431 - accuracy: 0.7014\n",
      "Epoch 49/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0425 - accuracy: 0.7080\n",
      "Epoch 50/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0415 - accuracy: 0.7152\n",
      "Epoch 51/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0416 - accuracy: 0.7147\n",
      "Epoch 52/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0409 - accuracy: 0.7196\n",
      "Epoch 53/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0404 - accuracy: 0.7242\n",
      "Epoch 54/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0396 - accuracy: 0.7310\n",
      "Epoch 55/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0394 - accuracy: 0.7328\n",
      "Epoch 56/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0385 - accuracy: 0.7395\n",
      "Epoch 57/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0383 - accuracy: 0.7408\n",
      "Epoch 58/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0379 - accuracy: 0.7440\n",
      "Epoch 59/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0375 - accuracy: 0.7449\n",
      "Epoch 60/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0368 - accuracy: 0.7517\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "Model accuracy: 0.2683333333333333\n",
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_9 (Conv1D)           (None, 69, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  (None, 34, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 1088)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               139392    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,810\n",
      "Trainable params: 140,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0894 - accuracy: 0.1457\n",
      "Epoch 2/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0876 - accuracy: 0.1958\n",
      "Epoch 3/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0865 - accuracy: 0.2252\n",
      "Epoch 4/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0853 - accuracy: 0.2502\n",
      "Epoch 5/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0843 - accuracy: 0.2679\n",
      "Epoch 6/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0830 - accuracy: 0.2852\n",
      "Epoch 7/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0820 - accuracy: 0.3060\n",
      "Epoch 8/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0806 - accuracy: 0.3273\n",
      "Epoch 9/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0794 - accuracy: 0.3455\n",
      "Epoch 10/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0780 - accuracy: 0.3607\n",
      "Epoch 11/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0768 - accuracy: 0.3760\n",
      "Epoch 12/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0754 - accuracy: 0.3947\n",
      "Epoch 13/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0740 - accuracy: 0.4131\n",
      "Epoch 14/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0726 - accuracy: 0.4285\n",
      "Epoch 15/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0713 - accuracy: 0.4426\n",
      "Epoch 16/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0700 - accuracy: 0.4605\n",
      "Epoch 17/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0688 - accuracy: 0.4706\n",
      "Epoch 18/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0674 - accuracy: 0.4846\n",
      "Epoch 19/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0665 - accuracy: 0.4948\n",
      "Epoch 20/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0651 - accuracy: 0.5111\n",
      "Epoch 21/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0639 - accuracy: 0.5242\n",
      "Epoch 22/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0629 - accuracy: 0.5297\n",
      "Epoch 23/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0620 - accuracy: 0.5395\n",
      "Epoch 24/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0608 - accuracy: 0.5525\n",
      "Epoch 25/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.5552\n",
      "Epoch 26/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0589 - accuracy: 0.5695\n",
      "Epoch 27/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0579 - accuracy: 0.5802\n",
      "Epoch 28/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0570 - accuracy: 0.5888\n",
      "Epoch 29/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0561 - accuracy: 0.5952\n",
      "Epoch 30/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0553 - accuracy: 0.6013\n",
      "Epoch 31/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0545 - accuracy: 0.6097\n",
      "Epoch 32/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0541 - accuracy: 0.6146\n",
      "Epoch 33/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0530 - accuracy: 0.6242\n",
      "Epoch 34/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0521 - accuracy: 0.6316\n",
      "Epoch 35/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.6358\n",
      "Epoch 36/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0508 - accuracy: 0.6417\n",
      "Epoch 37/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0498 - accuracy: 0.6509\n",
      "Epoch 38/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.6549\n",
      "Epoch 39/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0484 - accuracy: 0.6616\n",
      "Epoch 40/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0479 - accuracy: 0.6661\n",
      "Epoch 41/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0475 - accuracy: 0.6671\n",
      "Epoch 42/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0466 - accuracy: 0.6759\n",
      "Epoch 43/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0461 - accuracy: 0.6824\n",
      "Epoch 44/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0454 - accuracy: 0.6852\n",
      "Epoch 45/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0447 - accuracy: 0.6948\n",
      "Epoch 46/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0441 - accuracy: 0.6962\n",
      "Epoch 47/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0437 - accuracy: 0.7005\n",
      "Epoch 48/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0432 - accuracy: 0.7046\n",
      "Epoch 49/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0425 - accuracy: 0.7119\n",
      "Epoch 50/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0424 - accuracy: 0.7095\n",
      "Epoch 51/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0417 - accuracy: 0.7177\n",
      "Epoch 52/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0411 - accuracy: 0.7210\n",
      "Epoch 53/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0406 - accuracy: 0.7253\n",
      "Epoch 54/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0401 - accuracy: 0.7296\n",
      "Epoch 55/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0401 - accuracy: 0.7276\n",
      "Epoch 56/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0395 - accuracy: 0.7354\n",
      "Epoch 57/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0389 - accuracy: 0.7385\n",
      "Epoch 58/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0384 - accuracy: 0.7418\n",
      "Epoch 59/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0380 - accuracy: 0.7463\n",
      "Epoch 60/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0378 - accuracy: 0.7474\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "Model accuracy: 0.29333333333333333\n",
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_10 (Conv1D)          (None, 69, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPoolin  (None, 34, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 1088)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               139392    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,810\n",
      "Trainable params: 140,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0896 - accuracy: 0.1361\n",
      "Epoch 2/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0879 - accuracy: 0.1854\n",
      "Epoch 3/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0868 - accuracy: 0.2188\n",
      "Epoch 4/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0858 - accuracy: 0.2421\n",
      "Epoch 5/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0848 - accuracy: 0.2595\n",
      "Epoch 6/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.2813\n",
      "Epoch 7/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.2981\n",
      "Epoch 8/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0816 - accuracy: 0.3173\n",
      "Epoch 9/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0804 - accuracy: 0.3310\n",
      "Epoch 10/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.3474\n",
      "Epoch 11/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0779 - accuracy: 0.3676\n",
      "Epoch 12/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0765 - accuracy: 0.3830\n",
      "Epoch 13/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0754 - accuracy: 0.3969\n",
      "Epoch 14/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0740 - accuracy: 0.4150\n",
      "Epoch 15/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0726 - accuracy: 0.4298\n",
      "Epoch 16/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0715 - accuracy: 0.4426\n",
      "Epoch 17/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0701 - accuracy: 0.4594\n",
      "Epoch 18/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0689 - accuracy: 0.4705\n",
      "Epoch 19/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0677 - accuracy: 0.4852\n",
      "Epoch 20/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0665 - accuracy: 0.4930\n",
      "Epoch 21/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0654 - accuracy: 0.5116\n",
      "Epoch 22/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0640 - accuracy: 0.5217\n",
      "Epoch 23/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0629 - accuracy: 0.5326\n",
      "Epoch 24/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0619 - accuracy: 0.5392\n",
      "Epoch 25/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0607 - accuracy: 0.5536\n",
      "Epoch 26/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0597 - accuracy: 0.5638\n",
      "Epoch 27/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0585 - accuracy: 0.5715\n",
      "Epoch 28/60\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.0577 - accuracy: 0.5774\n",
      "Epoch 29/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0569 - accuracy: 0.5866\n",
      "Epoch 30/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0559 - accuracy: 0.5983\n",
      "Epoch 31/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0551 - accuracy: 0.6019\n",
      "Epoch 32/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0539 - accuracy: 0.6105\n",
      "Epoch 33/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0530 - accuracy: 0.6242\n",
      "Epoch 34/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0522 - accuracy: 0.6294\n",
      "Epoch 35/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0516 - accuracy: 0.6322\n",
      "Epoch 36/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0509 - accuracy: 0.6384\n",
      "Epoch 37/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0500 - accuracy: 0.6490\n",
      "Epoch 38/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0493 - accuracy: 0.6527\n",
      "Epoch 39/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0486 - accuracy: 0.6593\n",
      "Epoch 40/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0476 - accuracy: 0.6662\n",
      "Epoch 41/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0472 - accuracy: 0.6723\n",
      "Epoch 42/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0464 - accuracy: 0.6783\n",
      "Epoch 43/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0459 - accuracy: 0.6825\n",
      "Epoch 44/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0451 - accuracy: 0.6873\n",
      "Epoch 45/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0447 - accuracy: 0.6902\n",
      "Epoch 46/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0442 - accuracy: 0.6948\n",
      "Epoch 47/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0435 - accuracy: 0.7011\n",
      "Epoch 48/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0427 - accuracy: 0.7078\n",
      "Epoch 49/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0424 - accuracy: 0.7098\n",
      "Epoch 50/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0414 - accuracy: 0.7168\n",
      "Epoch 51/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0411 - accuracy: 0.7175\n",
      "Epoch 52/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0409 - accuracy: 0.7220\n",
      "Epoch 53/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0402 - accuracy: 0.7268\n",
      "Epoch 54/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0396 - accuracy: 0.7310\n",
      "Epoch 55/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0390 - accuracy: 0.7355\n",
      "Epoch 56/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0389 - accuracy: 0.7392\n",
      "Epoch 57/60\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.0383 - accuracy: 0.7430\n",
      "Epoch 58/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0375 - accuracy: 0.7487\n",
      "Epoch 59/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0373 - accuracy: 0.7506\n",
      "Epoch 60/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0368 - accuracy: 0.7540\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "Model accuracy: 0.278\n",
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_11 (Conv1D)          (None, 69, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 34, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 1088)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               139392    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,810\n",
      "Trainable params: 140,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "657/657 [==============================] - 3s 3ms/step - loss: 0.0896 - accuracy: 0.1416\n",
      "Epoch 2/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0880 - accuracy: 0.1850\n",
      "Epoch 3/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.2143\n",
      "Epoch 4/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0860 - accuracy: 0.2359\n",
      "Epoch 5/60\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 0.0851 - accuracy: 0.2545\n",
      "Epoch 6/60\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 0.0842 - accuracy: 0.2729\n",
      "Epoch 7/60\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 0.0832 - accuracy: 0.2910\n",
      "Epoch 8/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0822 - accuracy: 0.3045\n",
      "Epoch 9/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.3206\n",
      "Epoch 10/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.3399\n",
      "Epoch 11/60\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 0.0787 - accuracy: 0.3568\n",
      "Epoch 12/60\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 0.0775 - accuracy: 0.3707\n",
      "Epoch 13/60\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 0.0762 - accuracy: 0.3884\n",
      "Epoch 14/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0749 - accuracy: 0.4045\n",
      "Epoch 15/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0735 - accuracy: 0.4217\n",
      "Epoch 16/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.4341\n",
      "Epoch 17/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.4517\n",
      "Epoch 18/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0698 - accuracy: 0.4600\n",
      "Epoch 19/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0687 - accuracy: 0.4727\n",
      "Epoch 20/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.4886\n",
      "Epoch 21/60\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.5001\n",
      "Epoch 22/60\n",
      "434/657 [==================>...........] - ETA: 0s - loss: 0.0649 - accuracy: 0.5144"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     86\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train_encoded,\n\u001b[0;32m     88\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m,\n\u001b[0;32m     89\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[0;32m     91\u001b[0m \u001b[39m# Predicting the test set results\u001b[39;00m\n\u001b[0;32m     92\u001b[0m y_test_pred_prob \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1676\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1674\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1675\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1676\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   1677\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m         ):\n\u001b[0;32m   1684\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1376\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1377\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1378\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1380\u001b[0m )\n\u001b[0;32m   1382\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    646\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    648\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:774\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \n\u001b[0;32m    767\u001b[0m \u001b[39mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mRead\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 774\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n\u001b[0;32m    775\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:753\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    751\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 753\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    756\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    757\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m   tape\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    759\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    760\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    761\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:743\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    742\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 743\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    745\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    746\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:580\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    579\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 580\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    581\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    582\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    583\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "for i in range(10):\n",
    "    df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "    df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "    # Split df_features into X_train and X_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_features,\n",
    "        df_labels,\n",
    "        test_size=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(df_features.shape)\n",
    "    print(df_labels.shape)\n",
    "\n",
    "    # Assuming X_train is a Pandas Series\n",
    "    # Data augmentation - random perturbations\n",
    "    augmented_X_train = []\n",
    "    augmented_y_train = []\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        original_data = X_train.iloc[i].to_numpy()\n",
    "        augmented_X_train.append(original_data)\n",
    "        augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "        # Apply random perturbations\n",
    "        perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "        augmented_X_train.append(perturbed_data)\n",
    "        augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Convert augmented data to DataFrames\n",
    "    augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "    augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "    # Concatenate augmented data with original data\n",
    "    X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "    y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "    # Shuffle the augmented data\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "    # Reshape the data for CNN\n",
    "    X_train = X_train.values.reshape(-1, 71, 1)\n",
    "    X_test = X_test.values.reshape(-1, 71, 1)\n",
    "\n",
    "    # Define some constants\n",
    "    INPUT_SHAPE = (71, 1)  # Number of input features and channels\n",
    "    NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "    LEARNING_RATE = 0.0001  # Adjust as necessary\n",
    "\n",
    "    # One-hot encoding of output\n",
    "    y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "    # Define the CNN architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Compile the model with the custom optimizer\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train_encoded,\n",
    "                        epochs=60,\n",
    "                        batch_size=32)\n",
    "\n",
    "    # Predicting the test set results\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model accuracy:', accuracy)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    # confusion_mat = confusion_matrix(y_test, y_test_pred)\n",
    "    # print('Confusion Matrix:')    # print(confusion_mat)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase the complexity of the CNN and Ensemble methods\n",
    "#### Ensemble methods: Instead of training a single model, you can try using ensemble methods such as bagging or boosting. These techniques involve training multiple models and combining their predictions, which often leads to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39m# Train each model in the ensemble\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m ensemble_models:\n\u001b[1;32m---> 93\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train_encoded,\n\u001b[0;32m     94\u001b[0m               epochs\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m,\n\u001b[0;32m     95\u001b[0m               batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m     96\u001b[0m               verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# Set verbose=1 to see training progress\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m# Predicting the test set results using the ensemble\u001b[39;00m\n\u001b[0;32m     99\u001b[0m y_test_preds \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1676\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1674\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1675\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1676\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   1677\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m         ):\n\u001b[0;32m   1684\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1376\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1377\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1378\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1380\u001b[0m )\n\u001b[0;32m   1382\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    646\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    648\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:774\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \n\u001b[0;32m    767\u001b[0m \u001b[39mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mRead\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 774\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n\u001b[0;32m    775\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:753\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    751\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 753\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    756\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    757\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m   tape\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    759\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    760\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    761\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:743\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    742\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 743\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    745\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    746\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:580\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    579\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 580\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    581\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    582\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    583\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the training data and labels\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Assuming X_train is a Pandas Series\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "X_train = X_train.values.reshape(-1, 71, 1)\n",
    "X_test = X_test.values.reshape(-1, 71, 1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71, 1)  # Number of input features and channels\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.0001  # Adjust as necessary\n",
    "NUM_MODELS = 5  # Number of models in the ensemble\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the ensemble of models\n",
    "ensemble_models = []\n",
    "for _ in range(NUM_MODELS):\n",
    "    # Define the CNN architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Compile the model with the custom optimizer\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Add the model to the ensemble\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "# Train each model in the ensemble\n",
    "for model in ensemble_models:\n",
    "    model.fit(X_train, y_train_encoded,\n",
    "              epochs=30,\n",
    "              batch_size=32,\n",
    "              verbose=0)  # Set verbose=1 to see training progress\n",
    "\n",
    "# Predicting the test set results using the ensemble\n",
    "y_test_preds = []\n",
    "for model in ensemble_models:\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "# Take the majority vote from the ensemble predictions\n",
    "y_test_preds_ensemble = np.round(np.mean(y_test_preds, axis=0)).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_preds_ensemble)\n",
    "print('Ensemble Model Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 71)\n",
      "(10000, 1)\n",
      "Model 1 Training:\n",
      "Epoch 1/30\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 2.2892 - accuracy: 0.1265\n",
      "Epoch 2/30\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 2.2342 - accuracy: 0.1681\n",
      "Epoch 3/30\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 2.1875 - accuracy: 0.1886\n",
      "Epoch 4/30\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 2.1430 - accuracy: 0.2106\n",
      "Epoch 5/30\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 2.0964 - accuracy: 0.2299\n",
      "Epoch 6/30\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 2.0411 - accuracy: 0.2567\n",
      "Epoch 7/30\n",
      "157/657 [======>.......................] - ETA: 2s - loss: 2.0058 - accuracy: 0.2759"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m model_index, model \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ensemble_models):\n\u001b[0;32m     93\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m{\u001b[39;00mmodel_index\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Training:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train_encoded,\n\u001b[0;32m     95\u001b[0m                         epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[0;32m     96\u001b[0m                         batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m     97\u001b[0m                         verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# Set verbose=1 to see training progress\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# Predicting the test set results\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     y_test_pred_prob \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the training data and labels\n",
    "df_features = pd.read_csv('traindata.txt', delimiter=',', header=None)\n",
    "df_labels = pd.read_csv('trainlabels.txt', header=None)\n",
    "\n",
    "# Split df_features into X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features,\n",
    "    df_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_labels.shape)\n",
    "\n",
    "# Assuming X_train is a Pandas Series\n",
    "# Data augmentation - random perturbations\n",
    "augmented_X_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    original_data = X_train.iloc[i].to_numpy()\n",
    "    augmented_X_train.append(original_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "    # Apply random perturbations\n",
    "    perturbed_data = original_data + np.random.normal(0, 0.1, size=original_data.shape)\n",
    "    augmented_X_train.append(perturbed_data)\n",
    "    augmented_y_train.append(y_train.iloc[i].values[0])\n",
    "\n",
    "# Convert augmented data to DataFrames\n",
    "augmented_X_train = pd.DataFrame(augmented_X_train)\n",
    "augmented_y_train = pd.DataFrame(augmented_y_train)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "X_train = pd.concat([X_train, augmented_X_train], axis=0)\n",
    "y_train = pd.concat([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "X_train = X_train.values.reshape(-1, 71, 1)\n",
    "X_test = X_test.values.reshape(-1, 71, 1)\n",
    "\n",
    "# Define some constants\n",
    "INPUT_SHAPE = (71, 1)  # Number of input features and channels\n",
    "NUM_CLASSES = 10  # Number of output classes (0-9)\n",
    "LEARNING_RATE = 0.0001  # Adjust as necessary\n",
    "NUM_MODELS = 5  # Number of models in the ensemble\n",
    "\n",
    "# One-hot encoding of output\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Define the ensemble of models\n",
    "ensemble_models = []\n",
    "for _ in range(NUM_MODELS):\n",
    "    # Define the CNN architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Define an Adam optimizer with the desired learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Compile the model with the custom optimizer\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Append the model to the ensemble\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "# Train each model in the ensemble\n",
    "for model_index, model in enumerate(ensemble_models):\n",
    "    print(f\"Model {model_index + 1} Training:\")\n",
    "    history = model.fit(X_train, y_train_encoded,\n",
    "                        epochs=30,\n",
    "                        batch_size=32,\n",
    "                        verbose=1)  # Set verbose=1 to see training progress\n",
    "\n",
    "    # Predicting the test set results\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "    # Summary of the model\n",
    "    model.summary()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print('Model Accuracy:', accuracy)\n",
    "    print('Model Loss:', history.history['loss'][-1])\n",
    "    print()\n",
    "\n",
    "# Predicting the test set results using the ensemble\n",
    "y_test_preds = []\n",
    "for model in ensemble_models:\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "# Take the majority vote from the ensemble predictions\n",
    "y_test_preds_ensemble = np.round(np.mean(y_test_preds, axis=0)).astype(int)\n",
    "\n",
    "# Calculate accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, y_test_preds_ensemble)\n",
    "print('Ensemble Model Accuracy:', ensemble_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
